[04/26 21:24:18] fastreid INFO: Rank of current process: 0. World size: 1
[04/26 21:24:18] fastreid INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]
numpy                   1.23.5
fastreid                1.3 @/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid
FASTREID_ENV_MODULE     <not set>
PyTorch                 2.0.0+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0                   Tesla V100-PCIE-32GB
CUDA_HOME               /usr/local/cuda
Pillow                  11.1.0
torchvision             0.15.1+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torchvision
torchvision arch flags  sm_35, sm_50, sm_60, sm_70, sm_75, sm_80, sm_86
fvcore                  0.1.5.post20221221
cv2                     4.11.0
----------------------  ----------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/26 21:24:18] fastreid INFO: Command line arguments: Namespace(config_file='./configs/jk_experiments/agw-R101-ibn.yml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:60003', opts=[])
[04/26 21:24:18] fastreid INFO: Contents of args.config_file=./configs/jk_experiments/agw-R101-ibn.yml:
_BASE_: ../Base-AGW.yml

MODEL:
  BACKBONE:
    DEPTH: 101x
    WITH_IBN: True
  WEIGHTS: "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth"


DATASETS:
  NAMES: ("AIC24",)
  TESTS: ("AIC24",)

OUTPUT_DIR:  logs/jk_experiments/agw-R101-ibn

[04/26 21:24:18] fastreid INFO: Running with full config:
CUDNN_BENCHMARK: True
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER_TRAIN: NaiveIdentitySampler
  SET_WEIGHT: []
DATASETS:
  COMBINEALL: False
  NAMES: ('AIC24',)
  TESTS: ('AIC24',)
INPUT:
  AFFINE:
    ENABLED: False
  AUGMIX:
    ENABLED: False
    PROB: 0.0
  AUTOAUG:
    ENABLED: False
    PROB: 0.0
  CJ:
    BRIGHTNESS: 0.15
    CONTRAST: 0.15
    ENABLED: False
    HUE: 0.1
    PROB: 0.5
    SATURATION: 0.1
  CROP:
    ENABLED: False
    RATIO: [0.75, 1.3333333333333333]
    SCALE: [0.16, 1]
    SIZE: [224, 224]
  FLIP:
    ENABLED: True
    PROB: 0.5
  PADDING:
    ENABLED: True
    MODE: constant
    SIZE: 10
  REA:
    ENABLED: True
    PROB: 0.5
    VALUE: [123.675, 116.28, 103.53]
  RPT:
    ENABLED: False
    PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
KD:
  EMA:
    ENABLED: False
    MOMENTUM: 0.999
  MODEL_CONFIG: []
  MODEL_WEIGHTS: []
MODEL:
  BACKBONE:
    ATT_DROP_RATE: 0.0
    DEPTH: 101x
    DROP_PATH_RATIO: 0.1
    DROP_RATIO: 0.0
    FEAT_DIM: 2048
    LAST_STRIDE: 1
    NAME: build_resnet_backbone
    NORM: BN
    PRETRAIN: True
    PRETRAIN_PATH: 
    SIE_COE: 3.0
    STRIDE_SIZE: (16, 16)
    WITH_IBN: True
    WITH_NL: True
    WITH_SE: False
  DEVICE: cuda
  FREEZE_LAYERS: []
  HEADS:
    CLS_LAYER: Linear
    EMBEDDING_DIM: 0
    MARGIN: 0.0
    NAME: EmbeddingHead
    NECK_FEAT: before
    NORM: BN
    NUM_CLASSES: 0
    POOL_LAYER: GeneralizedMeanPooling
    SCALE: 1
    WITH_BNNECK: True
  LOSSES:
    CE:
      ALPHA: 0.2
      EPSILON: 0.1
      SCALE: 1.0
    CIRCLE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    COSFACE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    FL:
      ALPHA: 0.25
      GAMMA: 2
      SCALE: 1.0
    NAME: ('CrossEntropyLoss', 'TripletLoss')
    TRI:
      HARD_MINING: False
      MARGIN: 0.0
      NORM_FEAT: False
      SCALE: 1.0
  META_ARCHITECTURE: Baseline
  PIXEL_MEAN: [123.675, 116.28, 103.53]
  PIXEL_STD: [58.395, 57.120000000000005, 57.375]
  QUEUE_SIZE: 8192
  WEIGHTS: /WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth
OUTPUT_DIR: logs/jk_experiments/agw-R101-ibn
SOLVER:
  AMP:
    ENABLED: True
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 30
  CLIP_GRADIENTS:
    CLIP_TYPE: norm
    CLIP_VALUE: 5.0
    ENABLED: False
    NORM_TYPE: 2.0
  DELAY_EPOCHS: 0
  ETA_MIN_LR: 1e-07
  FREEZE_ITERS: 0
  GAMMA: 0.1
  HEADS_LR_FACTOR: 1.0
  IMS_PER_BATCH: 64
  MAX_EPOCH: 120
  MOMENTUM: 0.9
  NESTEROV: False
  OPT: Adam
  SCHED: MultiStepLR
  STEPS: [40, 90]
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 2000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005
  WEIGHT_DECAY_NORM: 0.0005
TEST:
  AQE:
    ALPHA: 3.0
    ENABLED: False
    QE_K: 5
    QE_TIME: 1
  EVAL_PERIOD: 30
  FLIP:
    ENABLED: False
  IMS_PER_BATCH: 128
  METRIC: cosine
  PRECISE_BN:
    DATASET: Market1501
    ENABLED: False
    NUM_ITER: 300
  RERANK:
    ENABLED: False
    K1: 20
    K2: 6
    LAMBDA: 0.3
  ROC:
    ENABLED: False
[04/26 21:24:18] fastreid INFO: Full config saved to /WAVE/users2/unix/jkou/PoseTrack/fast-reid/logs/jk_experiments/agw-R101-ibn/config.yaml
[04/26 21:24:18] fastreid.utils.env INFO: Using a generated random seed 22903116
[04/26 21:24:18] fastreid.engine.defaults INFO: Prepare training set
[04/26 21:24:19] fastreid.data.datasets.bases INFO: => Loaded AIC24 in csv format: 
| subset   | # ids   | # images   | # cameras   |
|:---------|:--------|:-----------|:------------|
| train    | 8000    | 132145     | 4           |
[04/26 21:24:19] fastreid.data.build INFO: Using training sampler NaiveIdentitySampler
[04/26 21:24:19] fastreid.engine.defaults INFO: Auto-scaling the num_classes=8000
[04/26 21:24:20] fastreid.modeling.backbones.resnet INFO: Some model parameters or buffers are not found in the checkpoint:
  layer1.0.bn1.IN.{weight, bias}
  layer1.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.1.bn1.IN.{weight, bias}
  layer1.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.2.bn1.IN.{weight, bias}
  layer1.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.0.bn1.IN.{weight, bias}
  layer2.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.1.bn1.IN.{weight, bias}
  layer2.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.2.bn1.IN.{weight, bias}
  layer2.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.3.bn1.IN.{weight, bias}
  layer2.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.0.bn1.IN.{weight, bias}
  layer3.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.1.bn1.IN.{weight, bias}
  layer3.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.2.bn1.IN.{weight, bias}
  layer3.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.3.bn1.IN.{weight, bias}
  layer3.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.4.bn1.IN.{weight, bias}
  layer3.4.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.5.bn1.IN.{weight, bias}
  layer3.5.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.6.bn1.IN.{weight, bias}
  layer3.6.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.7.bn1.IN.{weight, bias}
  layer3.7.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.8.bn1.IN.{weight, bias}
  layer3.8.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.9.bn1.IN.{weight, bias}
  layer3.9.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.10.bn1.IN.{weight, bias}
  layer3.10.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.11.bn1.IN.{weight, bias}
  layer3.11.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.12.bn1.IN.{weight, bias}
  layer3.12.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.13.bn1.IN.{weight, bias}
  layer3.13.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.14.bn1.IN.{weight, bias}
  layer3.14.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.15.bn1.IN.{weight, bias}
  layer3.15.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.16.bn1.IN.{weight, bias}
  layer3.16.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.17.bn1.IN.{weight, bias}
  layer3.17.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.18.bn1.IN.{weight, bias}
  layer3.18.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.19.bn1.IN.{weight, bias}
  layer3.19.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.20.bn1.IN.{weight, bias}
  layer3.20.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.21.bn1.IN.{weight, bias}
  layer3.21.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.22.bn1.IN.{weight, bias}
  layer3.22.bn1.BN.{weight, bias, running_mean, running_var}
  NL_2.0.g.{weight, bias}
  NL_2.0.W.0.{weight, bias}
  NL_2.0.W.1.{weight, bias, running_mean, running_var}
  NL_2.0.theta.{weight, bias}
  NL_2.0.phi.{weight, bias}
  NL_2.1.g.{weight, bias}
  NL_2.1.W.0.{weight, bias}
  NL_2.1.W.1.{weight, bias, running_mean, running_var}
  NL_2.1.theta.{weight, bias}
  NL_2.1.phi.{weight, bias}
  NL_3.0.g.{weight, bias}
  NL_3.0.W.0.{weight, bias}
  NL_3.0.W.1.{weight, bias, running_mean, running_var}
  NL_3.0.theta.{weight, bias}
  NL_3.0.phi.{weight, bias}
  NL_3.1.g.{weight, bias}
  NL_3.1.W.0.{weight, bias}
  NL_3.1.W.1.{weight, bias, running_mean, running_var}
  NL_3.1.theta.{weight, bias}
  NL_3.1.phi.{weight, bias}
  NL_3.2.g.{weight, bias}
  NL_3.2.W.0.{weight, bias}
  NL_3.2.W.1.{weight, bias, running_mean, running_var}
  NL_3.2.theta.{weight, bias}
  NL_3.2.phi.{weight, bias}
  NL_3.3.g.{weight, bias}
  NL_3.3.W.0.{weight, bias}
  NL_3.3.W.1.{weight, bias, running_mean, running_var}
  NL_3.3.theta.{weight, bias}
  NL_3.3.phi.{weight, bias}
  NL_3.4.g.{weight, bias}
  NL_3.4.W.0.{weight, bias}
  NL_3.4.W.1.{weight, bias, running_mean, running_var}
  NL_3.4.theta.{weight, bias}
  NL_3.4.phi.{weight, bias}
  NL_3.5.g.{weight, bias}
  NL_3.5.W.0.{weight, bias}
  NL_3.5.W.1.{weight, bias, running_mean, running_var}
  NL_3.5.theta.{weight, bias}
  NL_3.5.phi.{weight, bias}
  NL_3.6.g.{weight, bias}
  NL_3.6.W.0.{weight, bias}
  NL_3.6.W.1.{weight, bias, running_mean, running_var}
  NL_3.6.theta.{weight, bias}
  NL_3.6.phi.{weight, bias}
  NL_3.7.g.{weight, bias}
  NL_3.7.W.0.{weight, bias}
  NL_3.7.W.1.{weight, bias, running_mean, running_var}
  NL_3.7.theta.{weight, bias}
  NL_3.7.phi.{weight, bias}
  NL_3.8.g.{weight, bias}
  NL_3.8.W.0.{weight, bias}
  NL_3.8.W.1.{weight, bias, running_mean, running_var}
  NL_3.8.theta.{weight, bias}
  NL_3.8.phi.{weight, bias}
[04/26 21:24:20] fastreid.modeling.backbones.resnet INFO: The checkpoint state_dict contains keys that are not used by the model:
  layer1.0.bn1.{weight, bias, running_mean, running_var}
  layer1.1.bn1.{weight, bias, running_mean, running_var}
  layer1.2.bn1.{weight, bias, running_mean, running_var}
  layer2.0.bn1.{weight, bias, running_mean, running_var}
  layer2.1.bn1.{weight, bias, running_mean, running_var}
  layer2.2.bn1.{weight, bias, running_mean, running_var}
  layer2.3.bn1.{weight, bias, running_mean, running_var}
  layer3.0.bn1.{weight, bias, running_mean, running_var}
  layer3.1.bn1.{weight, bias, running_mean, running_var}
  layer3.2.bn1.{weight, bias, running_mean, running_var}
  layer3.3.bn1.{weight, bias, running_mean, running_var}
  layer3.4.bn1.{weight, bias, running_mean, running_var}
  layer3.5.bn1.{weight, bias, running_mean, running_var}
  layer3.6.bn1.{weight, bias, running_mean, running_var}
  layer3.7.bn1.{weight, bias, running_mean, running_var}
  layer3.8.bn1.{weight, bias, running_mean, running_var}
  layer3.9.bn1.{weight, bias, running_mean, running_var}
  layer3.10.bn1.{weight, bias, running_mean, running_var}
  layer3.11.bn1.{weight, bias, running_mean, running_var}
  layer3.12.bn1.{weight, bias, running_mean, running_var}
  layer3.13.bn1.{weight, bias, running_mean, running_var}
  layer3.14.bn1.{weight, bias, running_mean, running_var}
  layer3.15.bn1.{weight, bias, running_mean, running_var}
  layer3.16.bn1.{weight, bias, running_mean, running_var}
  layer3.17.bn1.{weight, bias, running_mean, running_var}
  layer3.18.bn1.{weight, bias, running_mean, running_var}
  layer3.19.bn1.{weight, bias, running_mean, running_var}
  layer3.20.bn1.{weight, bias, running_mean, running_var}
  layer3.21.bn1.{weight, bias, running_mean, running_var}
  layer3.22.bn1.{weight, bias, running_mean, running_var}
[04/26 21:24:21] fastreid.engine.defaults INFO: Model:
Baseline(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (NL_1): ModuleList()
    (NL_2): ModuleList(
      (0-1): 2 x Non_local(
        (g): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_3): ModuleList(
      (0-8): 9 x Non_local(
        (g): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_4): ModuleList()
  )
  (heads): EmbeddingHead(
    (pool_layer): GeneralizedMeanPooling(3.0, output_size=(1, 1))
    (bottleneck): Sequential(
      (0): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (cls_layer): Linear(num_classes=8000, scale=1, margin=0.0)
  )
)
[04/26 21:24:23] fastreid.utils.checkpoint INFO: Loading checkpoint from /WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth
[04/26 21:24:24] fastreid.utils.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  heads.bottleneck.0.{weight, running_mean, bias, running_var}
  heads.weight
[04/26 21:24:24] fastreid.utils.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  pixel_mean
  pixel_std
  heads.bnneck.{weight, bias, running_mean, running_var, num_batches_tracked}
  heads.classifier.weight
  heads.pool_layer.p
[04/26 21:24:24] fastreid.engine.train_loop INFO: Starting training from epoch 0
[04/26 21:24:54] fastreid.utils.events INFO:  eta: 9:05:51  epoch/iter: 0/199  total_loss: 9.097  loss_cls: 8.941  loss_triplet: 0.1614  time: 0.1325  data_time: 0.0031  lr: 6.63e-05  max_mem: 8616M
[04/26 21:25:21] fastreid.utils.events INFO:  eta: 9:03:29  epoch/iter: 0/399  total_loss: 8.691  loss_cls: 8.609  loss_triplet: 0.07507  time: 0.1321  data_time: 0.0026  lr: 9.78e-05  max_mem: 8616M
[04/26 21:25:48] fastreid.utils.events INFO:  eta: 9:02:53  epoch/iter: 0/599  total_loss: 8.245  loss_cls: 8.174  loss_triplet: 0.06167  time: 0.1319  data_time: 0.0032  lr: 1.29e-04  max_mem: 8616M
[04/26 21:26:16] fastreid.utils.events INFO:  eta: 9:01:05  epoch/iter: 0/799  total_loss: 7.705  loss_cls: 7.64  loss_triplet: 0.0466  time: 0.1317  data_time: 0.0029  lr: 1.61e-04  max_mem: 8616M
[04/26 21:26:43] fastreid.utils.events INFO:  eta: 9:00:14  epoch/iter: 0/999  total_loss: 7.146  loss_cls: 7.101  loss_triplet: 0.04813  time: 0.1317  data_time: 0.0028  lr: 1.92e-04  max_mem: 8616M
[04/26 21:27:10] fastreid.utils.events INFO:  eta: 8:59:08  epoch/iter: 0/1199  total_loss: 6.667  loss_cls: 6.609  loss_triplet: 0.0444  time: 0.1316  data_time: 0.0024  lr: 2.24e-04  max_mem: 8616M
[04/26 21:27:37] fastreid.utils.events INFO:  eta: 8:58:06  epoch/iter: 0/1399  total_loss: 6.086  loss_cls: 6.039  loss_triplet: 0.04087  time: 0.1316  data_time: 0.0020  lr: 2.55e-04  max_mem: 8616M
[04/26 21:28:04] fastreid.utils.events INFO:  eta: 8:56:33  epoch/iter: 0/1599  total_loss: 5.196  loss_cls: 5.143  loss_triplet: 0.04044  time: 0.1314  data_time: 0.0021  lr: 2.87e-04  max_mem: 8616M
[04/26 21:28:30] fastreid.utils.events INFO:  eta: 8:55:22  epoch/iter: 0/1799  total_loss: 4.379  loss_cls: 4.339  loss_triplet: 0.03738  time: 0.1313  data_time: 0.0010  lr: 3.18e-04  max_mem: 8616M
[04/26 21:28:57] fastreid.utils.events INFO:  eta: 8:53:53  epoch/iter: 0/1999  total_loss: 3.829  loss_cls: 3.798  loss_triplet: 0.03834  time: 0.1312  data_time: 0.0029  lr: 3.50e-04  max_mem: 8616M
[04/26 21:29:06] fastreid.utils.events INFO:  eta: 8:54:38  epoch/iter: 0/2063  total_loss: 3.958  loss_cls: 3.936  loss_triplet: 0.03632  time: 0.1313  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:29:24] fastreid.utils.events INFO:  eta: 8:55:51  epoch/iter: 1/2199  total_loss: 3.755  loss_cls: 3.708  loss_triplet: 0.03584  time: 0.1315  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:29:51] fastreid.utils.events INFO:  eta: 8:59:25  epoch/iter: 1/2399  total_loss: 3.401  loss_cls: 3.357  loss_triplet: 0.03235  time: 0.1318  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:30:18] fastreid.utils.events INFO:  eta: 9:03:07  epoch/iter: 1/2599  total_loss: 3.146  loss_cls: 3.111  loss_triplet: 0.03285  time: 0.1320  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:30:45] fastreid.utils.events INFO:  eta: 9:06:04  epoch/iter: 1/2799  total_loss: 2.987  loss_cls: 2.943  loss_triplet: 0.02988  time: 0.1322  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:31:12] fastreid.utils.events INFO:  eta: 9:07:56  epoch/iter: 1/2999  total_loss: 2.847  loss_cls: 2.814  loss_triplet: 0.0307  time: 0.1324  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 21:31:39] fastreid.utils.events INFO:  eta: 9:07:27  epoch/iter: 1/3199  total_loss: 2.636  loss_cls: 2.608  loss_triplet: 0.02632  time: 0.1326  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 21:32:06] fastreid.utils.events INFO:  eta: 9:06:46  epoch/iter: 1/3399  total_loss: 2.542  loss_cls: 2.505  loss_triplet: 0.02691  time: 0.1327  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 21:32:33] fastreid.utils.events INFO:  eta: 9:05:42  epoch/iter: 1/3599  total_loss: 2.362  loss_cls: 2.324  loss_triplet: 0.02662  time: 0.1327  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 21:32:59] fastreid.utils.events INFO:  eta: 9:03:44  epoch/iter: 1/3799  total_loss: 2.061  loss_cls: 2.03  loss_triplet: 0.02347  time: 0.1328  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:33:26] fastreid.utils.events INFO:  eta: 9:03:18  epoch/iter: 1/3999  total_loss: 2.108  loss_cls: 2.086  loss_triplet: 0.02158  time: 0.1329  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:33:44] fastreid.utils.events INFO:  eta: 9:02:58  epoch/iter: 1/4127  total_loss: 2.104  loss_cls: 2.077  loss_triplet: 0.02147  time: 0.1329  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 21:33:53] fastreid.utils.events INFO:  eta: 9:02:51  epoch/iter: 2/4199  total_loss: 2.1  loss_cls: 2.072  loss_triplet: 0.0243  time: 0.1329  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:34:20] fastreid.utils.events INFO:  eta: 9:02:45  epoch/iter: 2/4399  total_loss: 2.119  loss_cls: 2.101  loss_triplet: 0.02295  time: 0.1330  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:34:47] fastreid.utils.events INFO:  eta: 9:02:54  epoch/iter: 2/4599  total_loss: 2.152  loss_cls: 2.124  loss_triplet: 0.0238  time: 0.1331  data_time: 0.0031  lr: 3.50e-04  max_mem: 8616M
[04/26 21:35:14] fastreid.utils.events INFO:  eta: 9:03:51  epoch/iter: 2/4799  total_loss: 2.119  loss_cls: 2.095  loss_triplet: 0.02457  time: 0.1332  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:35:41] fastreid.utils.events INFO:  eta: 9:03:30  epoch/iter: 2/4999  total_loss: 2.14  loss_cls: 2.111  loss_triplet: 0.02227  time: 0.1332  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 21:36:08] fastreid.utils.events INFO:  eta: 9:02:46  epoch/iter: 2/5199  total_loss: 2.1  loss_cls: 2.072  loss_triplet: 0.02303  time: 0.1333  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 21:36:35] fastreid.utils.events INFO:  eta: 9:01:42  epoch/iter: 2/5399  total_loss: 2.091  loss_cls: 2.059  loss_triplet: 0.02346  time: 0.1333  data_time: 0.0016  lr: 3.50e-04  max_mem: 8616M
[04/26 21:37:01] fastreid.utils.events INFO:  eta: 8:59:33  epoch/iter: 2/5599  total_loss: 1.982  loss_cls: 1.956  loss_triplet: 0.02453  time: 0.1333  data_time: 0.0008  lr: 3.50e-04  max_mem: 8616M
[04/26 21:37:28] fastreid.utils.events INFO:  eta: 8:58:53  epoch/iter: 2/5799  total_loss: 1.958  loss_cls: 1.939  loss_triplet: 0.02043  time: 0.1333  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:37:55] fastreid.utils.events INFO:  eta: 8:58:35  epoch/iter: 2/5999  total_loss: 2.029  loss_cls: 2.001  loss_triplet: 0.02079  time: 0.1334  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:38:21] fastreid.utils.events INFO:  eta: 8:58:36  epoch/iter: 2/6191  total_loss: 2.045  loss_cls: 2.02  loss_triplet: 0.02057  time: 0.1334  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:38:22] fastreid.utils.events INFO:  eta: 8:58:36  epoch/iter: 3/6199  total_loss: 2.047  loss_cls: 2.026  loss_triplet: 0.02057  time: 0.1334  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 21:38:49] fastreid.utils.events INFO:  eta: 8:59:16  epoch/iter: 3/6399  total_loss: 2.039  loss_cls: 2.018  loss_triplet: 0.02282  time: 0.1335  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:39:16] fastreid.utils.events INFO:  eta: 9:00:08  epoch/iter: 3/6599  total_loss: 2.08  loss_cls: 2.063  loss_triplet: 0.02315  time: 0.1335  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:39:43] fastreid.utils.events INFO:  eta: 8:59:31  epoch/iter: 3/6799  total_loss: 2.059  loss_cls: 2.036  loss_triplet: 0.02107  time: 0.1335  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:40:10] fastreid.utils.events INFO:  eta: 8:58:52  epoch/iter: 3/6999  total_loss: 2.067  loss_cls: 2.037  loss_triplet: 0.02163  time: 0.1336  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 21:40:37] fastreid.utils.events INFO:  eta: 8:57:26  epoch/iter: 3/7199  total_loss: 2.02  loss_cls: 1.998  loss_triplet: 0.01947  time: 0.1336  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 21:41:03] fastreid.utils.events INFO:  eta: 8:56:07  epoch/iter: 3/7399  total_loss: 1.965  loss_cls: 1.937  loss_triplet: 0.02084  time: 0.1336  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 21:41:30] fastreid.utils.events INFO:  eta: 8:55:10  epoch/iter: 3/7599  total_loss: 1.873  loss_cls: 1.849  loss_triplet: 0.01893  time: 0.1336  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:41:57] fastreid.utils.events INFO:  eta: 8:55:11  epoch/iter: 3/7799  total_loss: 1.983  loss_cls: 1.963  loss_triplet: 0.01769  time: 0.1336  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:42:24] fastreid.utils.events INFO:  eta: 8:55:15  epoch/iter: 3/7999  total_loss: 2.012  loss_cls: 1.99  loss_triplet: 0.0206  time: 0.1336  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 21:42:51] fastreid.utils.events INFO:  eta: 8:55:19  epoch/iter: 3/8199  total_loss: 1.985  loss_cls: 1.969  loss_triplet: 0.01846  time: 0.1337  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:42:59] fastreid.utils.events INFO:  eta: 8:55:29  epoch/iter: 3/8255  total_loss: 2.003  loss_cls: 1.981  loss_triplet: 0.01902  time: 0.1337  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:43:18] fastreid.utils.events INFO:  eta: 8:55:58  epoch/iter: 4/8399  total_loss: 2.033  loss_cls: 2.014  loss_triplet: 0.01874  time: 0.1337  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 21:43:45] fastreid.utils.events INFO:  eta: 8:55:56  epoch/iter: 4/8599  total_loss: 2.022  loss_cls: 2.004  loss_triplet: 0.01844  time: 0.1337  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:44:12] fastreid.utils.events INFO:  eta: 8:55:31  epoch/iter: 4/8799  total_loss: 2.008  loss_cls: 1.988  loss_triplet: 0.01847  time: 0.1337  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 21:44:39] fastreid.utils.events INFO:  eta: 8:54:34  epoch/iter: 4/8999  total_loss: 2.035  loss_cls: 2.011  loss_triplet: 0.01884  time: 0.1337  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 21:45:06] fastreid.utils.events INFO:  eta: 8:53:55  epoch/iter: 4/9199  total_loss: 1.955  loss_cls: 1.928  loss_triplet: 0.02083  time: 0.1338  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 21:45:33] fastreid.utils.events INFO:  eta: 8:53:26  epoch/iter: 4/9399  total_loss: 1.855  loss_cls: 1.826  loss_triplet: 0.0221  time: 0.1338  data_time: 0.0009  lr: 3.50e-04  max_mem: 8616M
[04/26 21:46:00] fastreid.utils.events INFO:  eta: 8:53:06  epoch/iter: 4/9599  total_loss: 1.925  loss_cls: 1.907  loss_triplet: 0.01865  time: 0.1338  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 21:46:27] fastreid.utils.events INFO:  eta: 8:52:48  epoch/iter: 4/9799  total_loss: 1.962  loss_cls: 1.936  loss_triplet: 0.01856  time: 0.1339  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 21:46:54] fastreid.utils.events INFO:  eta: 8:52:49  epoch/iter: 4/9999  total_loss: 1.957  loss_cls: 1.934  loss_triplet: 0.01739  time: 0.1339  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:47:21] fastreid.utils.events INFO:  eta: 8:52:25  epoch/iter: 4/10199  total_loss: 2.004  loss_cls: 1.978  loss_triplet: 0.01805  time: 0.1339  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:47:37] fastreid.utils.events INFO:  eta: 8:52:36  epoch/iter: 4/10319  total_loss: 1.996  loss_cls: 1.969  loss_triplet: 0.01833  time: 0.1339  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:47:48] fastreid.utils.events INFO:  eta: 8:52:26  epoch/iter: 5/10399  total_loss: 1.997  loss_cls: 1.975  loss_triplet: 0.0196  time: 0.1339  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:48:15] fastreid.utils.events INFO:  eta: 8:52:12  epoch/iter: 5/10599  total_loss: 1.997  loss_cls: 1.974  loss_triplet: 0.01754  time: 0.1339  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:48:42] fastreid.utils.events INFO:  eta: 8:51:32  epoch/iter: 5/10799  total_loss: 1.987  loss_cls: 1.954  loss_triplet: 0.01765  time: 0.1339  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 21:49:09] fastreid.utils.events INFO:  eta: 8:50:51  epoch/iter: 5/10999  total_loss: 1.944  loss_cls: 1.919  loss_triplet: 0.01954  time: 0.1340  data_time: 0.0015  lr: 3.50e-04  max_mem: 8616M
[04/26 21:49:36] fastreid.utils.events INFO:  eta: 8:49:16  epoch/iter: 5/11199  total_loss: 1.869  loss_cls: 1.848  loss_triplet: 0.0187  time: 0.1340  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 21:50:03] fastreid.utils.events INFO:  eta: 8:48:32  epoch/iter: 5/11399  total_loss: 1.816  loss_cls: 1.794  loss_triplet: 0.01784  time: 0.1340  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:50:30] fastreid.utils.events INFO:  eta: 8:47:56  epoch/iter: 5/11599  total_loss: 1.894  loss_cls: 1.875  loss_triplet: 0.01676  time: 0.1340  data_time: 0.0029  lr: 3.50e-04  max_mem: 8616M
[04/26 21:50:57] fastreid.utils.events INFO:  eta: 8:48:22  epoch/iter: 5/11799  total_loss: 1.918  loss_cls: 1.902  loss_triplet: 0.01705  time: 0.1340  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:51:24] fastreid.utils.events INFO:  eta: 8:48:21  epoch/iter: 5/11999  total_loss: 1.926  loss_cls: 1.903  loss_triplet: 0.01722  time: 0.1340  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:51:51] fastreid.utils.events INFO:  eta: 8:49:18  epoch/iter: 5/12199  total_loss: 1.936  loss_cls: 1.913  loss_triplet: 0.01878  time: 0.1341  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:52:16] fastreid.utils.events INFO:  eta: 8:48:34  epoch/iter: 5/12383  total_loss: 1.991  loss_cls: 1.971  loss_triplet: 0.01817  time: 0.1341  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 21:52:18] fastreid.utils.events INFO:  eta: 8:48:29  epoch/iter: 6/12399  total_loss: 1.991  loss_cls: 1.97  loss_triplet: 0.0178  time: 0.1341  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 21:52:45] fastreid.utils.events INFO:  eta: 8:47:27  epoch/iter: 6/12599  total_loss: 1.937  loss_cls: 1.914  loss_triplet: 0.01692  time: 0.1341  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 21:53:12] fastreid.utils.events INFO:  eta: 8:45:58  epoch/iter: 6/12799  total_loss: 1.935  loss_cls: 1.914  loss_triplet: 0.017  time: 0.1341  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 21:53:38] fastreid.utils.events INFO:  eta: 8:44:45  epoch/iter: 6/12999  total_loss: 1.903  loss_cls: 1.875  loss_triplet: 0.01858  time: 0.1341  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 21:54:05] fastreid.utils.events INFO:  eta: 8:42:59  epoch/iter: 6/13199  total_loss: 1.755  loss_cls: 1.732  loss_triplet: 0.01831  time: 0.1341  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:54:32] fastreid.utils.events INFO:  eta: 8:43:01  epoch/iter: 6/13399  total_loss: 1.863  loss_cls: 1.842  loss_triplet: 0.01652  time: 0.1341  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:54:59] fastreid.utils.events INFO:  eta: 8:42:44  epoch/iter: 6/13599  total_loss: 1.906  loss_cls: 1.889  loss_triplet: 0.01613  time: 0.1341  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 21:55:26] fastreid.utils.events INFO:  eta: 8:43:25  epoch/iter: 6/13799  total_loss: 1.916  loss_cls: 1.898  loss_triplet: 0.01546  time: 0.1341  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:55:53] fastreid.utils.events INFO:  eta: 8:44:05  epoch/iter: 6/13999  total_loss: 1.937  loss_cls: 1.915  loss_triplet: 0.01658  time: 0.1341  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:56:20] fastreid.utils.events INFO:  eta: 8:45:30  epoch/iter: 6/14199  total_loss: 1.936  loss_cls: 1.915  loss_triplet: 0.0179  time: 0.1341  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 21:56:48] fastreid.utils.events INFO:  eta: 8:45:44  epoch/iter: 6/14399  total_loss: 1.933  loss_cls: 1.911  loss_triplet: 0.01664  time: 0.1342  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 21:56:54] fastreid.utils.events INFO:  eta: 8:45:42  epoch/iter: 6/14447  total_loss: 1.919  loss_cls: 1.9  loss_triplet: 0.01778  time: 0.1342  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:57:15] fastreid.utils.events INFO:  eta: 8:45:41  epoch/iter: 7/14599  total_loss: 1.923  loss_cls: 1.898  loss_triplet: 0.0167  time: 0.1342  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:57:41] fastreid.utils.events INFO:  eta: 8:43:52  epoch/iter: 7/14799  total_loss: 1.88  loss_cls: 1.852  loss_triplet: 0.01665  time: 0.1342  data_time: 0.0015  lr: 3.50e-04  max_mem: 8616M
[04/26 21:58:08] fastreid.utils.events INFO:  eta: 8:41:52  epoch/iter: 7/14999  total_loss: 1.788  loss_cls: 1.77  loss_triplet: 0.01963  time: 0.1342  data_time: 0.0009  lr: 3.50e-04  max_mem: 8616M
[04/26 21:58:35] fastreid.utils.events INFO:  eta: 8:40:22  epoch/iter: 7/15199  total_loss: 1.773  loss_cls: 1.757  loss_triplet: 0.01479  time: 0.1342  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 21:59:02] fastreid.utils.events INFO:  eta: 8:39:30  epoch/iter: 7/15399  total_loss: 1.857  loss_cls: 1.833  loss_triplet: 0.01707  time: 0.1342  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 21:59:29] fastreid.utils.events INFO:  eta: 8:38:21  epoch/iter: 7/15599  total_loss: 1.881  loss_cls: 1.867  loss_triplet: 0.01757  time: 0.1342  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 21:59:56] fastreid.utils.events INFO:  eta: 8:38:32  epoch/iter: 7/15799  total_loss: 1.862  loss_cls: 1.846  loss_triplet: 0.01529  time: 0.1342  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:00:23] fastreid.utils.events INFO:  eta: 8:38:54  epoch/iter: 7/15999  total_loss: 1.864  loss_cls: 1.851  loss_triplet: 0.01726  time: 0.1342  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:00:50] fastreid.utils.events INFO:  eta: 8:38:26  epoch/iter: 7/16199  total_loss: 1.907  loss_cls: 1.884  loss_triplet: 0.01623  time: 0.1342  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 22:01:17] fastreid.utils.events INFO:  eta: 8:37:38  epoch/iter: 7/16399  total_loss: 1.912  loss_cls: 1.891  loss_triplet: 0.01957  time: 0.1342  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:01:32] fastreid.utils.events INFO:  eta: 8:37:55  epoch/iter: 7/16511  total_loss: 1.894  loss_cls: 1.874  loss_triplet: 0.01905  time: 0.1342  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:01:44] fastreid.utils.events INFO:  eta: 8:37:41  epoch/iter: 8/16599  total_loss: 1.906  loss_cls: 1.885  loss_triplet: 0.01773  time: 0.1342  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 22:02:11] fastreid.utils.events INFO:  eta: 8:37:07  epoch/iter: 8/16799  total_loss: 1.848  loss_cls: 1.822  loss_triplet: 0.01588  time: 0.1342  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 22:02:38] fastreid.utils.events INFO:  eta: 8:35:55  epoch/iter: 8/16999  total_loss: 1.708  loss_cls: 1.69  loss_triplet: 0.01551  time: 0.1342  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:03:05] fastreid.utils.events INFO:  eta: 8:35:42  epoch/iter: 8/17199  total_loss: 1.819  loss_cls: 1.802  loss_triplet: 0.01536  time: 0.1342  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:03:32] fastreid.utils.events INFO:  eta: 8:35:59  epoch/iter: 8/17399  total_loss: 1.841  loss_cls: 1.828  loss_triplet: 0.01538  time: 0.1343  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:03:59] fastreid.utils.events INFO:  eta: 8:36:05  epoch/iter: 8/17599  total_loss: 1.888  loss_cls: 1.863  loss_triplet: 0.01667  time: 0.1343  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:04:26] fastreid.utils.events INFO:  eta: 8:36:54  epoch/iter: 8/17799  total_loss: 1.866  loss_cls: 1.853  loss_triplet: 0.01612  time: 0.1343  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:04:53] fastreid.utils.events INFO:  eta: 8:36:58  epoch/iter: 8/17999  total_loss: 1.878  loss_cls: 1.863  loss_triplet: 0.01475  time: 0.1343  data_time: 0.0029  lr: 3.50e-04  max_mem: 8616M
[04/26 22:05:20] fastreid.utils.events INFO:  eta: 8:36:32  epoch/iter: 8/18199  total_loss: 1.85  loss_cls: 1.821  loss_triplet: 0.0147  time: 0.1343  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:05:47] fastreid.utils.events INFO:  eta: 8:35:35  epoch/iter: 8/18399  total_loss: 1.89  loss_cls: 1.868  loss_triplet: 0.01578  time: 0.1343  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 22:06:11] fastreid.utils.events INFO:  eta: 8:34:36  epoch/iter: 8/18575  total_loss: 1.864  loss_cls: 1.842  loss_triplet: 0.01468  time: 0.1343  data_time: 0.0015  lr: 3.50e-04  max_mem: 8616M
[04/26 22:06:14] fastreid.utils.events INFO:  eta: 8:34:31  epoch/iter: 9/18599  total_loss: 1.86  loss_cls: 1.833  loss_triplet: 0.01503  time: 0.1343  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:06:41] fastreid.utils.events INFO:  eta: 8:32:24  epoch/iter: 9/18799  total_loss: 1.793  loss_cls: 1.772  loss_triplet: 0.01921  time: 0.1343  data_time: 0.0009  lr: 3.50e-04  max_mem: 8616M
[04/26 22:07:08] fastreid.utils.events INFO:  eta: 8:32:16  epoch/iter: 9/18999  total_loss: 1.775  loss_cls: 1.753  loss_triplet: 0.01709  time: 0.1343  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:07:35] fastreid.utils.events INFO:  eta: 8:32:18  epoch/iter: 9/19199  total_loss: 1.857  loss_cls: 1.83  loss_triplet: 0.01333  time: 0.1343  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:08:02] fastreid.utils.events INFO:  eta: 8:32:36  epoch/iter: 9/19399  total_loss: 1.831  loss_cls: 1.805  loss_triplet: 0.0159  time: 0.1343  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:08:29] fastreid.utils.events INFO:  eta: 8:32:27  epoch/iter: 9/19599  total_loss: 1.885  loss_cls: 1.86  loss_triplet: 0.01491  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:08:56] fastreid.utils.events INFO:  eta: 8:33:13  epoch/iter: 9/19799  total_loss: 1.847  loss_cls: 1.826  loss_triplet: 0.01655  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:09:24] fastreid.utils.events INFO:  eta: 8:33:19  epoch/iter: 9/19999  total_loss: 1.862  loss_cls: 1.836  loss_triplet: 0.01497  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:09:51] fastreid.utils.events INFO:  eta: 8:32:28  epoch/iter: 9/20199  total_loss: 1.874  loss_cls: 1.854  loss_triplet: 0.01534  time: 0.1344  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 22:10:17] fastreid.utils.events INFO:  eta: 8:30:35  epoch/iter: 9/20399  total_loss: 1.879  loss_cls: 1.856  loss_triplet: 0.01697  time: 0.1344  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 22:10:44] fastreid.utils.events INFO:  eta: 8:28:59  epoch/iter: 9/20599  total_loss: 1.757  loss_cls: 1.736  loss_triplet: 0.01594  time: 0.1344  data_time: 0.0010  lr: 3.50e-04  max_mem: 8616M
[04/26 22:10:50] fastreid.utils.events INFO:  eta: 8:28:27  epoch/iter: 9/20639  total_loss: 1.752  loss_cls: 1.734  loss_triplet: 0.0164  time: 0.1344  data_time: 0.0011  lr: 3.50e-04  max_mem: 8616M
[04/26 22:11:11] fastreid.utils.events INFO:  eta: 8:27:23  epoch/iter: 10/20799  total_loss: 1.725  loss_cls: 1.708  loss_triplet: 0.01633  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:11:38] fastreid.utils.events INFO:  eta: 8:26:26  epoch/iter: 10/20999  total_loss: 1.839  loss_cls: 1.822  loss_triplet: 0.01509  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:12:05] fastreid.utils.events INFO:  eta: 8:26:01  epoch/iter: 10/21199  total_loss: 1.82  loss_cls: 1.802  loss_triplet: 0.01499  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:12:32] fastreid.utils.events INFO:  eta: 8:26:02  epoch/iter: 10/21399  total_loss: 1.854  loss_cls: 1.835  loss_triplet: 0.01581  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:12:59] fastreid.utils.events INFO:  eta: 8:26:06  epoch/iter: 10/21599  total_loss: 1.854  loss_cls: 1.835  loss_triplet: 0.0152  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:13:26] fastreid.utils.events INFO:  eta: 8:26:25  epoch/iter: 10/21799  total_loss: 1.864  loss_cls: 1.847  loss_triplet: 0.01488  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:13:53] fastreid.utils.events INFO:  eta: 8:25:35  epoch/iter: 10/21999  total_loss: 1.847  loss_cls: 1.822  loss_triplet: 0.0156  time: 0.1344  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 22:14:20] fastreid.utils.events INFO:  eta: 8:24:34  epoch/iter: 10/22199  total_loss: 1.865  loss_cls: 1.841  loss_triplet: 0.0153  time: 0.1344  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 22:14:47] fastreid.utils.events INFO:  eta: 8:23:29  epoch/iter: 10/22399  total_loss: 1.815  loss_cls: 1.788  loss_triplet: 0.01705  time: 0.1344  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 22:15:13] fastreid.utils.events INFO:  eta: 8:22:04  epoch/iter: 10/22599  total_loss: 1.71  loss_cls: 1.693  loss_triplet: 0.01576  time: 0.1344  data_time: 0.0007  lr: 3.50e-04  max_mem: 8616M
[04/26 22:15:28] fastreid.utils.events INFO:  eta: 8:21:49  epoch/iter: 10/22703  total_loss: 1.703  loss_cls: 1.685  loss_triplet: 0.01547  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:15:40] fastreid.utils.events INFO:  eta: 8:21:09  epoch/iter: 11/22799  total_loss: 1.767  loss_cls: 1.76  loss_triplet: 0.01561  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:16:07] fastreid.utils.events INFO:  eta: 8:21:12  epoch/iter: 11/22999  total_loss: 1.823  loss_cls: 1.805  loss_triplet: 0.01643  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:16:34] fastreid.utils.events INFO:  eta: 8:21:06  epoch/iter: 11/23199  total_loss: 1.822  loss_cls: 1.811  loss_triplet: 0.01481  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:17:01] fastreid.utils.events INFO:  eta: 8:21:25  epoch/iter: 11/23399  total_loss: 1.85  loss_cls: 1.827  loss_triplet: 0.0162  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:17:28] fastreid.utils.events INFO:  eta: 8:22:07  epoch/iter: 11/23599  total_loss: 1.831  loss_cls: 1.811  loss_triplet: 0.01555  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:17:55] fastreid.utils.events INFO:  eta: 8:22:01  epoch/iter: 11/23799  total_loss: 1.872  loss_cls: 1.852  loss_triplet: 0.01585  time: 0.1344  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 22:18:22] fastreid.utils.events INFO:  eta: 8:21:21  epoch/iter: 11/23999  total_loss: 1.834  loss_cls: 1.817  loss_triplet: 0.01478  time: 0.1344  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 22:18:49] fastreid.utils.events INFO:  eta: 8:20:30  epoch/iter: 11/24199  total_loss: 1.806  loss_cls: 1.787  loss_triplet: 0.01368  time: 0.1344  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:19:16] fastreid.utils.events INFO:  eta: 8:19:16  epoch/iter: 11/24399  total_loss: 1.731  loss_cls: 1.712  loss_triplet: 0.01611  time: 0.1344  data_time: 0.0010  lr: 3.50e-04  max_mem: 8616M
[04/26 22:19:43] fastreid.utils.events INFO:  eta: 8:18:21  epoch/iter: 11/24599  total_loss: 1.701  loss_cls: 1.686  loss_triplet: 0.01672  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:20:06] fastreid.utils.events INFO:  eta: 8:18:02  epoch/iter: 11/24767  total_loss: 1.797  loss_cls: 1.775  loss_triplet: 0.01463  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:20:10] fastreid.utils.events INFO:  eta: 8:18:01  epoch/iter: 12/24799  total_loss: 1.793  loss_cls: 1.774  loss_triplet: 0.01423  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:20:37] fastreid.utils.events INFO:  eta: 8:17:35  epoch/iter: 12/24999  total_loss: 1.787  loss_cls: 1.773  loss_triplet: 0.01442  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:21:04] fastreid.utils.events INFO:  eta: 8:17:34  epoch/iter: 12/25199  total_loss: 1.851  loss_cls: 1.83  loss_triplet: 0.01637  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:21:31] fastreid.utils.events INFO:  eta: 8:18:15  epoch/iter: 12/25399  total_loss: 1.802  loss_cls: 1.794  loss_triplet: 0.01336  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:21:58] fastreid.utils.events INFO:  eta: 8:18:17  epoch/iter: 12/25599  total_loss: 1.85  loss_cls: 1.823  loss_triplet: 0.01553  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:22:25] fastreid.utils.events INFO:  eta: 8:17:34  epoch/iter: 12/25799  total_loss: 1.823  loss_cls: 1.808  loss_triplet: 0.01374  time: 0.1345  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 22:22:52] fastreid.utils.events INFO:  eta: 8:16:59  epoch/iter: 12/25999  total_loss: 1.797  loss_cls: 1.779  loss_triplet: 0.0156  time: 0.1345  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:23:19] fastreid.utils.events INFO:  eta: 8:15:24  epoch/iter: 12/26199  total_loss: 1.786  loss_cls: 1.769  loss_triplet: 0.01707  time: 0.1344  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 22:23:45] fastreid.utils.events INFO:  eta: 8:14:07  epoch/iter: 12/26399  total_loss: 1.689  loss_cls: 1.672  loss_triplet: 0.01625  time: 0.1344  data_time: 0.0034  lr: 3.50e-04  max_mem: 8616M
[04/26 22:24:13] fastreid.utils.events INFO:  eta: 8:13:50  epoch/iter: 12/26599  total_loss: 1.792  loss_cls: 1.772  loss_triplet: 0.01383  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:24:40] fastreid.utils.events INFO:  eta: 8:13:50  epoch/iter: 12/26799  total_loss: 1.807  loss_cls: 1.782  loss_triplet: 0.01376  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:24:44] fastreid.utils.events INFO:  eta: 8:13:53  epoch/iter: 12/26831  total_loss: 1.815  loss_cls: 1.799  loss_triplet: 0.01357  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:25:07] fastreid.utils.events INFO:  eta: 8:13:40  epoch/iter: 13/26999  total_loss: 1.81  loss_cls: 1.798  loss_triplet: 0.01359  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:25:34] fastreid.utils.events INFO:  eta: 8:14:25  epoch/iter: 13/27199  total_loss: 1.79  loss_cls: 1.77  loss_triplet: 0.01478  time: 0.1345  data_time: 0.0029  lr: 3.50e-04  max_mem: 8616M
[04/26 22:26:01] fastreid.utils.events INFO:  eta: 8:14:27  epoch/iter: 13/27399  total_loss: 1.807  loss_cls: 1.794  loss_triplet: 0.01311  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:26:27] fastreid.utils.events INFO:  eta: 8:13:39  epoch/iter: 13/27599  total_loss: 1.845  loss_cls: 1.832  loss_triplet: 0.01517  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 22:26:54] fastreid.utils.events INFO:  eta: 8:12:56  epoch/iter: 13/27799  total_loss: 1.848  loss_cls: 1.827  loss_triplet: 0.01545  time: 0.1345  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:27:21] fastreid.utils.events INFO:  eta: 8:11:54  epoch/iter: 13/27999  total_loss: 1.776  loss_cls: 1.759  loss_triplet: 0.01492  time: 0.1345  data_time: 0.0015  lr: 3.50e-04  max_mem: 8616M
[04/26 22:27:48] fastreid.utils.events INFO:  eta: 8:10:11  epoch/iter: 13/28199  total_loss: 1.741  loss_cls: 1.726  loss_triplet: 0.01501  time: 0.1345  data_time: 0.0013  lr: 3.50e-04  max_mem: 8616M
[04/26 22:28:15] fastreid.utils.events INFO:  eta: 8:09:05  epoch/iter: 13/28399  total_loss: 1.703  loss_cls: 1.687  loss_triplet: 0.01476  time: 0.1345  data_time: 0.0029  lr: 3.50e-04  max_mem: 8616M
[04/26 22:28:42] fastreid.utils.events INFO:  eta: 8:08:54  epoch/iter: 13/28599  total_loss: 1.793  loss_cls: 1.773  loss_triplet: 0.01454  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:29:09] fastreid.utils.events INFO:  eta: 8:08:41  epoch/iter: 13/28799  total_loss: 1.794  loss_cls: 1.778  loss_triplet: 0.01387  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:29:22] fastreid.utils.events INFO:  eta: 8:08:38  epoch/iter: 13/28895  total_loss: 1.786  loss_cls: 1.772  loss_triplet: 0.01448  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:29:36] fastreid.utils.events INFO:  eta: 8:08:42  epoch/iter: 14/28999  total_loss: 1.807  loss_cls: 1.788  loss_triplet: 0.01339  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:30:03] fastreid.utils.events INFO:  eta: 8:09:20  epoch/iter: 14/29199  total_loss: 1.786  loss_cls: 1.773  loss_triplet: 0.01464  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:30:29] fastreid.utils.events INFO:  eta: 8:08:56  epoch/iter: 14/29399  total_loss: 1.812  loss_cls: 1.796  loss_triplet: 0.01434  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 22:30:56] fastreid.utils.events INFO:  eta: 8:08:18  epoch/iter: 14/29599  total_loss: 1.806  loss_cls: 1.788  loss_triplet: 0.01595  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 22:31:23] fastreid.utils.events INFO:  eta: 8:07:18  epoch/iter: 14/29799  total_loss: 1.827  loss_cls: 1.805  loss_triplet: 0.01583  time: 0.1345  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 22:31:50] fastreid.utils.events INFO:  eta: 8:06:08  epoch/iter: 14/29999  total_loss: 1.747  loss_cls: 1.732  loss_triplet: 0.01533  time: 0.1345  data_time: 0.0016  lr: 3.50e-04  max_mem: 8616M
[04/26 22:32:17] fastreid.utils.events INFO:  eta: 8:04:51  epoch/iter: 14/30199  total_loss: 1.666  loss_cls: 1.645  loss_triplet: 0.01548  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:32:44] fastreid.utils.events INFO:  eta: 8:04:35  epoch/iter: 14/30399  total_loss: 1.746  loss_cls: 1.728  loss_triplet: 0.01309  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:33:10] fastreid.utils.events INFO:  eta: 8:04:00  epoch/iter: 14/30599  total_loss: 1.782  loss_cls: 1.767  loss_triplet: 0.01397  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:33:37] fastreid.utils.events INFO:  eta: 8:04:01  epoch/iter: 14/30799  total_loss: 1.783  loss_cls: 1.759  loss_triplet: 0.0151  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:33:59] fastreid.utils.events INFO:  eta: 8:04:15  epoch/iter: 14/30959  total_loss: 1.787  loss_cls: 1.767  loss_triplet: 0.013  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:34:04] fastreid.utils.events INFO:  eta: 8:04:13  epoch/iter: 15/30999  total_loss: 1.794  loss_cls: 1.775  loss_triplet: 0.01333  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:34:31] fastreid.utils.events INFO:  eta: 8:04:26  epoch/iter: 15/31199  total_loss: 1.764  loss_cls: 1.744  loss_triplet: 0.01498  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:34:58] fastreid.utils.events INFO:  eta: 8:03:58  epoch/iter: 15/31399  total_loss: 1.822  loss_cls: 1.803  loss_triplet: 0.0152  time: 0.1345  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 22:35:25] fastreid.utils.events INFO:  eta: 8:03:45  epoch/iter: 15/31599  total_loss: 1.817  loss_cls: 1.795  loss_triplet: 0.01463  time: 0.1345  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 22:35:52] fastreid.utils.events INFO:  eta: 8:02:32  epoch/iter: 15/31799  total_loss: 1.764  loss_cls: 1.744  loss_triplet: 0.01415  time: 0.1344  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 22:36:19] fastreid.utils.events INFO:  eta: 8:01:25  epoch/iter: 15/31999  total_loss: 1.71  loss_cls: 1.697  loss_triplet: 0.01451  time: 0.1344  data_time: 0.0009  lr: 3.50e-04  max_mem: 8616M
[04/26 22:36:46] fastreid.utils.events INFO:  eta: 8:00:28  epoch/iter: 15/32199  total_loss: 1.69  loss_cls: 1.673  loss_triplet: 0.01234  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:37:13] fastreid.utils.events INFO:  eta: 7:59:50  epoch/iter: 15/32399  total_loss: 1.781  loss_cls: 1.761  loss_triplet: 0.01482  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:37:39] fastreid.utils.events INFO:  eta: 7:59:44  epoch/iter: 15/32599  total_loss: 1.784  loss_cls: 1.768  loss_triplet: 0.01466  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:38:06] fastreid.utils.events INFO:  eta: 8:00:06  epoch/iter: 15/32799  total_loss: 1.777  loss_cls: 1.757  loss_triplet: 0.01434  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:38:33] fastreid.utils.events INFO:  eta: 8:00:30  epoch/iter: 15/32999  total_loss: 1.779  loss_cls: 1.76  loss_triplet: 0.01283  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:38:36] fastreid.utils.events INFO:  eta: 8:00:30  epoch/iter: 15/33023  total_loss: 1.79  loss_cls: 1.775  loss_triplet: 0.01268  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:39:00] fastreid.utils.events INFO:  eta: 8:00:12  epoch/iter: 16/33199  total_loss: 1.796  loss_cls: 1.782  loss_triplet: 0.01444  time: 0.1344  data_time: 0.0031  lr: 3.50e-04  max_mem: 8616M
[04/26 22:39:27] fastreid.utils.events INFO:  eta: 7:59:36  epoch/iter: 16/33399  total_loss: 1.767  loss_cls: 1.749  loss_triplet: 0.01394  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:39:54] fastreid.utils.events INFO:  eta: 7:58:27  epoch/iter: 16/33599  total_loss: 1.781  loss_cls: 1.77  loss_triplet: 0.01562  time: 0.1344  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 22:40:21] fastreid.utils.events INFO:  eta: 7:57:32  epoch/iter: 16/33799  total_loss: 1.754  loss_cls: 1.738  loss_triplet: 0.01643  time: 0.1344  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 22:40:47] fastreid.utils.events INFO:  eta: 7:56:33  epoch/iter: 16/33999  total_loss: 1.659  loss_cls: 1.64  loss_triplet: 0.01471  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:41:14] fastreid.utils.events INFO:  eta: 7:56:06  epoch/iter: 16/34199  total_loss: 1.735  loss_cls: 1.71  loss_triplet: 0.01431  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:41:41] fastreid.utils.events INFO:  eta: 7:56:08  epoch/iter: 16/34399  total_loss: 1.742  loss_cls: 1.725  loss_triplet: 0.01262  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:42:08] fastreid.utils.events INFO:  eta: 7:56:44  epoch/iter: 16/34599  total_loss: 1.749  loss_cls: 1.728  loss_triplet: 0.01404  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:42:35] fastreid.utils.events INFO:  eta: 7:56:50  epoch/iter: 16/34799  total_loss: 1.792  loss_cls: 1.777  loss_triplet: 0.01485  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:43:02] fastreid.utils.events INFO:  eta: 7:57:06  epoch/iter: 16/34999  total_loss: 1.776  loss_cls: 1.761  loss_triplet: 0.0141  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:43:14] fastreid.utils.events INFO:  eta: 7:57:24  epoch/iter: 16/35087  total_loss: 1.785  loss_cls: 1.765  loss_triplet: 0.01416  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:43:29] fastreid.utils.events INFO:  eta: 7:57:10  epoch/iter: 17/35199  total_loss: 1.772  loss_cls: 1.753  loss_triplet: 0.01341  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:43:56] fastreid.utils.events INFO:  eta: 7:56:19  epoch/iter: 17/35399  total_loss: 1.768  loss_cls: 1.745  loss_triplet: 0.01483  time: 0.1344  data_time: 0.0016  lr: 3.50e-04  max_mem: 8616M
[04/26 22:44:23] fastreid.utils.events INFO:  eta: 7:54:58  epoch/iter: 17/35599  total_loss: 1.761  loss_cls: 1.736  loss_triplet: 0.01433  time: 0.1344  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 22:44:50] fastreid.utils.events INFO:  eta: 7:53:18  epoch/iter: 17/35799  total_loss: 1.656  loss_cls: 1.633  loss_triplet: 0.01429  time: 0.1344  data_time: 0.0007  lr: 3.50e-04  max_mem: 8616M
[04/26 22:45:17] fastreid.utils.events INFO:  eta: 7:52:52  epoch/iter: 17/35999  total_loss: 1.714  loss_cls: 1.696  loss_triplet: 0.01387  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:45:43] fastreid.utils.events INFO:  eta: 7:52:07  epoch/iter: 17/36199  total_loss: 1.738  loss_cls: 1.726  loss_triplet: 0.01252  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:46:10] fastreid.utils.events INFO:  eta: 7:52:12  epoch/iter: 17/36399  total_loss: 1.758  loss_cls: 1.733  loss_triplet: 0.01387  time: 0.1344  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 22:46:37] fastreid.utils.events INFO:  eta: 7:51:54  epoch/iter: 17/36599  total_loss: 1.727  loss_cls: 1.706  loss_triplet: 0.01395  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:47:04] fastreid.utils.events INFO:  eta: 7:52:13  epoch/iter: 17/36799  total_loss: 1.781  loss_cls: 1.771  loss_triplet: 0.01499  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:47:31] fastreid.utils.events INFO:  eta: 7:52:03  epoch/iter: 17/36999  total_loss: 1.787  loss_cls: 1.766  loss_triplet: 0.01446  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:47:52] fastreid.utils.events INFO:  eta: 7:51:35  epoch/iter: 17/37151  total_loss: 1.788  loss_cls: 1.767  loss_triplet: 0.01479  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:47:58] fastreid.utils.events INFO:  eta: 7:51:23  epoch/iter: 18/37199  total_loss: 1.781  loss_cls: 1.765  loss_triplet: 0.01495  time: 0.1344  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:48:25] fastreid.utils.events INFO:  eta: 7:50:17  epoch/iter: 18/37399  total_loss: 1.76  loss_cls: 1.741  loss_triplet: 0.01385  time: 0.1344  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 22:48:52] fastreid.utils.events INFO:  eta: 7:49:20  epoch/iter: 18/37599  total_loss: 1.709  loss_cls: 1.698  loss_triplet: 0.01478  time: 0.1344  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 22:49:18] fastreid.utils.events INFO:  eta: 7:48:25  epoch/iter: 18/37799  total_loss: 1.652  loss_cls: 1.638  loss_triplet: 0.01477  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:49:45] fastreid.utils.events INFO:  eta: 7:47:42  epoch/iter: 18/37999  total_loss: 1.715  loss_cls: 1.697  loss_triplet: 0.01285  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 22:50:12] fastreid.utils.events INFO:  eta: 7:47:33  epoch/iter: 18/38199  total_loss: 1.738  loss_cls: 1.724  loss_triplet: 0.01264  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:50:39] fastreid.utils.events INFO:  eta: 7:47:39  epoch/iter: 18/38399  total_loss: 1.726  loss_cls: 1.711  loss_triplet: 0.01267  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:51:06] fastreid.utils.events INFO:  eta: 7:48:09  epoch/iter: 18/38599  total_loss: 1.781  loss_cls: 1.763  loss_triplet: 0.01423  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 22:51:33] fastreid.utils.events INFO:  eta: 7:48:53  epoch/iter: 18/38799  total_loss: 1.791  loss_cls: 1.773  loss_triplet: 0.01129  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:52:00] fastreid.utils.events INFO:  eta: 7:48:26  epoch/iter: 18/38999  total_loss: 1.781  loss_cls: 1.762  loss_triplet: 0.0145  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 22:52:27] fastreid.utils.events INFO:  eta: 7:47:42  epoch/iter: 18/39199  total_loss: 1.742  loss_cls: 1.73  loss_triplet: 0.01496  time: 0.1344  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 22:52:29] fastreid.utils.events INFO:  eta: 7:47:32  epoch/iter: 18/39215  total_loss: 1.735  loss_cls: 1.724  loss_triplet: 0.01496  time: 0.1344  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 22:52:54] fastreid.utils.events INFO:  eta: 7:46:15  epoch/iter: 19/39399  total_loss: 1.723  loss_cls: 1.704  loss_triplet: 0.014  time: 0.1344  data_time: 0.0013  lr: 3.50e-04  max_mem: 8616M
[04/26 22:53:20] fastreid.utils.events INFO:  eta: 7:44:48  epoch/iter: 19/39599  total_loss: 1.612  loss_cls: 1.59  loss_triplet: 0.01354  time: 0.1344  data_time: 0.0033  lr: 3.50e-04  max_mem: 8616M
[04/26 22:53:47] fastreid.utils.events INFO:  eta: 7:43:59  epoch/iter: 19/39799  total_loss: 1.711  loss_cls: 1.694  loss_triplet: 0.0139  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:54:14] fastreid.utils.events INFO:  eta: 7:43:43  epoch/iter: 19/39999  total_loss: 1.731  loss_cls: 1.718  loss_triplet: 0.01477  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:54:41] fastreid.utils.events INFO:  eta: 7:43:45  epoch/iter: 19/40199  total_loss: 1.728  loss_cls: 1.716  loss_triplet: 0.01241  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:55:08] fastreid.utils.events INFO:  eta: 7:44:19  epoch/iter: 19/40399  total_loss: 1.78  loss_cls: 1.764  loss_triplet: 0.01379  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:55:35] fastreid.utils.events INFO:  eta: 7:44:41  epoch/iter: 19/40599  total_loss: 1.743  loss_cls: 1.727  loss_triplet: 0.01221  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:56:02] fastreid.utils.events INFO:  eta: 7:44:02  epoch/iter: 19/40799  total_loss: 1.726  loss_cls: 1.708  loss_triplet: 0.01365  time: 0.1344  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 22:56:29] fastreid.utils.events INFO:  eta: 7:43:23  epoch/iter: 19/40999  total_loss: 1.765  loss_cls: 1.741  loss_triplet: 0.01251  time: 0.1344  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 22:56:56] fastreid.utils.events INFO:  eta: 7:42:15  epoch/iter: 19/41199  total_loss: 1.742  loss_cls: 1.722  loss_triplet: 0.01477  time: 0.1344  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 22:57:07] fastreid.utils.events INFO:  eta: 7:41:45  epoch/iter: 19/41279  total_loss: 1.718  loss_cls: 1.699  loss_triplet: 0.01538  time: 0.1344  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 22:57:23] fastreid.utils.events INFO:  eta: 7:40:44  epoch/iter: 20/41399  total_loss: 1.661  loss_cls: 1.641  loss_triplet: 0.01455  time: 0.1344  data_time: 0.0011  lr: 3.50e-04  max_mem: 8616M
[04/26 22:57:50] fastreid.utils.events INFO:  eta: 7:40:19  epoch/iter: 20/41599  total_loss: 1.657  loss_cls: 1.64  loss_triplet: 0.01339  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:58:17] fastreid.utils.events INFO:  eta: 7:40:09  epoch/iter: 20/41799  total_loss: 1.721  loss_cls: 1.698  loss_triplet: 0.01384  time: 0.1344  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 22:58:44] fastreid.utils.events INFO:  eta: 7:40:12  epoch/iter: 20/41999  total_loss: 1.724  loss_cls: 1.703  loss_triplet: 0.01302  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 22:59:11] fastreid.utils.events INFO:  eta: 7:40:36  epoch/iter: 20/42199  total_loss: 1.723  loss_cls: 1.708  loss_triplet: 0.01487  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 22:59:38] fastreid.utils.events INFO:  eta: 7:41:11  epoch/iter: 20/42399  total_loss: 1.734  loss_cls: 1.715  loss_triplet: 0.01409  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:00:05] fastreid.utils.events INFO:  eta: 7:40:23  epoch/iter: 20/42599  total_loss: 1.732  loss_cls: 1.713  loss_triplet: 0.01349  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 23:00:31] fastreid.utils.events INFO:  eta: 7:39:23  epoch/iter: 20/42799  total_loss: 1.747  loss_cls: 1.726  loss_triplet: 0.01476  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:00:58] fastreid.utils.events INFO:  eta: 7:38:18  epoch/iter: 20/42999  total_loss: 1.761  loss_cls: 1.738  loss_triplet: 0.01558  time: 0.1344  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 23:01:25] fastreid.utils.events INFO:  eta: 7:37:07  epoch/iter: 20/43199  total_loss: 1.708  loss_cls: 1.688  loss_triplet: 0.01363  time: 0.1344  data_time: 0.0013  lr: 3.50e-04  max_mem: 8616M
[04/26 23:01:44] fastreid.utils.events INFO:  eta: 7:36:13  epoch/iter: 20/43343  total_loss: 1.658  loss_cls: 1.64  loss_triplet: 0.01522  time: 0.1344  data_time: 0.0008  lr: 3.50e-04  max_mem: 8616M
[04/26 23:01:52] fastreid.utils.events INFO:  eta: 7:36:02  epoch/iter: 21/43399  total_loss: 1.639  loss_cls: 1.618  loss_triplet: 0.01542  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:02:19] fastreid.utils.events INFO:  eta: 7:36:12  epoch/iter: 21/43599  total_loss: 1.684  loss_cls: 1.671  loss_triplet: 0.01212  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:02:46] fastreid.utils.events INFO:  eta: 7:36:12  epoch/iter: 21/43799  total_loss: 1.714  loss_cls: 1.701  loss_triplet: 0.01285  time: 0.1344  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:03:13] fastreid.utils.events INFO:  eta: 7:36:00  epoch/iter: 21/43999  total_loss: 1.705  loss_cls: 1.69  loss_triplet: 0.01367  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:03:40] fastreid.utils.events INFO:  eta: 7:36:21  epoch/iter: 21/44199  total_loss: 1.723  loss_cls: 1.704  loss_triplet: 0.01282  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:04:07] fastreid.utils.events INFO:  eta: 7:36:28  epoch/iter: 21/44399  total_loss: 1.722  loss_cls: 1.709  loss_triplet: 0.0138  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:04:34] fastreid.utils.events INFO:  eta: 7:35:46  epoch/iter: 21/44599  total_loss: 1.731  loss_cls: 1.713  loss_triplet: 0.01413  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:05:01] fastreid.utils.events INFO:  eta: 7:34:56  epoch/iter: 21/44799  total_loss: 1.732  loss_cls: 1.714  loss_triplet: 0.01405  time: 0.1345  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 23:05:27] fastreid.utils.events INFO:  eta: 7:33:26  epoch/iter: 21/44999  total_loss: 1.68  loss_cls: 1.661  loss_triplet: 0.01214  time: 0.1344  data_time: 0.0013  lr: 3.50e-04  max_mem: 8616M
[04/26 23:05:54] fastreid.utils.events INFO:  eta: 7:32:13  epoch/iter: 21/45199  total_loss: 1.65  loss_cls: 1.63  loss_triplet: 0.0152  time: 0.1344  data_time: 0.0009  lr: 3.50e-04  max_mem: 8616M
[04/26 23:06:21] fastreid.utils.events INFO:  eta: 7:31:36  epoch/iter: 21/45399  total_loss: 1.662  loss_cls: 1.645  loss_triplet: 0.0128  time: 0.1344  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:06:22] fastreid.utils.events INFO:  eta: 7:31:35  epoch/iter: 21/45407  total_loss: 1.667  loss_cls: 1.653  loss_triplet: 0.01284  time: 0.1344  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 23:06:48] fastreid.utils.events INFO:  eta: 7:31:17  epoch/iter: 22/45599  total_loss: 1.714  loss_cls: 1.697  loss_triplet: 0.0126  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:07:15] fastreid.utils.events INFO:  eta: 7:31:09  epoch/iter: 22/45799  total_loss: 1.694  loss_cls: 1.678  loss_triplet: 0.01408  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:07:42] fastreid.utils.events INFO:  eta: 7:31:36  epoch/iter: 22/45999  total_loss: 1.727  loss_cls: 1.708  loss_triplet: 0.01486  time: 0.1344  data_time: 0.0032  lr: 3.50e-04  max_mem: 8616M
[04/26 23:08:09] fastreid.utils.events INFO:  eta: 7:31:55  epoch/iter: 22/46199  total_loss: 1.748  loss_cls: 1.724  loss_triplet: 0.01218  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:08:36] fastreid.utils.events INFO:  eta: 7:31:25  epoch/iter: 22/46399  total_loss: 1.718  loss_cls: 1.704  loss_triplet: 0.01328  time: 0.1345  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 23:09:03] fastreid.utils.events INFO:  eta: 7:30:54  epoch/iter: 22/46599  total_loss: 1.733  loss_cls: 1.717  loss_triplet: 0.01307  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:09:29] fastreid.utils.events INFO:  eta: 7:29:53  epoch/iter: 22/46799  total_loss: 1.743  loss_cls: 1.72  loss_triplet: 0.01461  time: 0.1345  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 23:09:56] fastreid.utils.events INFO:  eta: 7:28:51  epoch/iter: 22/46999  total_loss: 1.666  loss_cls: 1.649  loss_triplet: 0.01257  time: 0.1344  data_time: 0.0011  lr: 3.50e-04  max_mem: 8616M
[04/26 23:10:23] fastreid.utils.events INFO:  eta: 7:27:44  epoch/iter: 22/47199  total_loss: 1.614  loss_cls: 1.597  loss_triplet: 0.01437  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:10:50] fastreid.utils.events INFO:  eta: 7:27:37  epoch/iter: 22/47399  total_loss: 1.692  loss_cls: 1.677  loss_triplet: 0.0123  time: 0.1344  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:11:00] fastreid.utils.events INFO:  eta: 7:27:30  epoch/iter: 22/47471  total_loss: 1.698  loss_cls: 1.68  loss_triplet: 0.01222  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:11:17] fastreid.utils.events INFO:  eta: 7:27:24  epoch/iter: 23/47599  total_loss: 1.702  loss_cls: 1.684  loss_triplet: 0.01286  time: 0.1345  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 23:11:44] fastreid.utils.events INFO:  eta: 7:27:29  epoch/iter: 23/47799  total_loss: 1.706  loss_cls: 1.685  loss_triplet: 0.01338  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:12:11] fastreid.utils.events INFO:  eta: 7:27:46  epoch/iter: 23/47999  total_loss: 1.73  loss_cls: 1.714  loss_triplet: 0.01464  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:12:38] fastreid.utils.events INFO:  eta: 7:27:36  epoch/iter: 23/48199  total_loss: 1.713  loss_cls: 1.695  loss_triplet: 0.01292  time: 0.1345  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 23:13:05] fastreid.utils.events INFO:  eta: 7:26:42  epoch/iter: 23/48399  total_loss: 1.729  loss_cls: 1.708  loss_triplet: 0.01389  time: 0.1345  data_time: 0.0022  lr: 3.50e-04  max_mem: 8616M
[04/26 23:13:32] fastreid.utils.events INFO:  eta: 7:25:53  epoch/iter: 23/48599  total_loss: 1.694  loss_cls: 1.675  loss_triplet: 0.01333  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 23:13:59] fastreid.utils.events INFO:  eta: 7:24:52  epoch/iter: 23/48799  total_loss: 1.688  loss_cls: 1.671  loss_triplet: 0.01229  time: 0.1345  data_time: 0.0015  lr: 3.50e-04  max_mem: 8616M
[04/26 23:14:25] fastreid.utils.events INFO:  eta: 7:23:13  epoch/iter: 23/48999  total_loss: 1.623  loss_cls: 1.606  loss_triplet: 0.01433  time: 0.1344  data_time: 0.0007  lr: 3.50e-04  max_mem: 8616M
[04/26 23:14:52] fastreid.utils.events INFO:  eta: 7:22:49  epoch/iter: 23/49199  total_loss: 1.653  loss_cls: 1.647  loss_triplet: 0.01382  time: 0.1344  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:15:19] fastreid.utils.events INFO:  eta: 7:22:36  epoch/iter: 23/49399  total_loss: 1.696  loss_cls: 1.676  loss_triplet: 0.01284  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:15:37] fastreid.utils.events INFO:  eta: 7:22:39  epoch/iter: 23/49535  total_loss: 1.692  loss_cls: 1.668  loss_triplet: 0.01343  time: 0.1345  data_time: 0.0032  lr: 3.50e-04  max_mem: 8616M
[04/26 23:15:46] fastreid.utils.events INFO:  eta: 7:22:48  epoch/iter: 24/49599  total_loss: 1.674  loss_cls: 1.661  loss_triplet: 0.01408  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:16:13] fastreid.utils.events INFO:  eta: 7:22:50  epoch/iter: 24/49799  total_loss: 1.703  loss_cls: 1.69  loss_triplet: 0.0133  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:16:40] fastreid.utils.events INFO:  eta: 7:23:22  epoch/iter: 24/49999  total_loss: 1.715  loss_cls: 1.692  loss_triplet: 0.01327  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:17:07] fastreid.utils.events INFO:  eta: 7:23:18  epoch/iter: 24/50199  total_loss: 1.714  loss_cls: 1.7  loss_triplet: 0.0118  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:17:34] fastreid.utils.events INFO:  eta: 7:22:25  epoch/iter: 24/50399  total_loss: 1.684  loss_cls: 1.659  loss_triplet: 0.0137  time: 0.1345  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 23:18:01] fastreid.utils.events INFO:  eta: 7:21:20  epoch/iter: 24/50599  total_loss: 1.699  loss_cls: 1.685  loss_triplet: 0.01322  time: 0.1345  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 23:18:27] fastreid.utils.events INFO:  eta: 7:20:01  epoch/iter: 24/50799  total_loss: 1.661  loss_cls: 1.643  loss_triplet: 0.01348  time: 0.1345  data_time: 0.0010  lr: 3.50e-04  max_mem: 8616M
[04/26 23:18:54] fastreid.utils.events INFO:  eta: 7:19:33  epoch/iter: 24/50999  total_loss: 1.616  loss_cls: 1.599  loss_triplet: 0.01483  time: 0.1344  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:19:21] fastreid.utils.events INFO:  eta: 7:19:03  epoch/iter: 24/51199  total_loss: 1.676  loss_cls: 1.658  loss_triplet: 0.0128  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:19:48] fastreid.utils.events INFO:  eta: 7:18:58  epoch/iter: 24/51399  total_loss: 1.671  loss_cls: 1.653  loss_triplet: 0.01278  time: 0.1345  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 23:20:15] fastreid.utils.events INFO:  eta: 7:19:02  epoch/iter: 24/51599  total_loss: 1.677  loss_cls: 1.66  loss_triplet: 0.01274  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:20:15] fastreid.utils.events INFO:  eta: 7:19:02  epoch/iter: 24/51599  total_loss: 1.677  loss_cls: 1.66  loss_triplet: 0.01274  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:20:42] fastreid.utils.events INFO:  eta: 7:19:28  epoch/iter: 25/51799  total_loss: 1.719  loss_cls: 1.703  loss_triplet: 0.01443  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:21:09] fastreid.utils.events INFO:  eta: 7:19:34  epoch/iter: 25/51999  total_loss: 1.717  loss_cls: 1.703  loss_triplet: 0.01397  time: 0.1345  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 23:21:36] fastreid.utils.events INFO:  eta: 7:18:38  epoch/iter: 25/52199  total_loss: 1.716  loss_cls: 1.701  loss_triplet: 0.01203  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:22:03] fastreid.utils.events INFO:  eta: 7:17:48  epoch/iter: 25/52399  total_loss: 1.718  loss_cls: 1.712  loss_triplet: 0.01356  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 23:22:30] fastreid.utils.events INFO:  eta: 7:16:37  epoch/iter: 25/52599  total_loss: 1.655  loss_cls: 1.64  loss_triplet: 0.01356  time: 0.1345  data_time: 0.0012  lr: 3.50e-04  max_mem: 8616M
[04/26 23:22:56] fastreid.utils.events INFO:  eta: 7:15:17  epoch/iter: 25/52799  total_loss: 1.607  loss_cls: 1.589  loss_triplet: 0.01488  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:23:23] fastreid.utils.events INFO:  eta: 7:14:41  epoch/iter: 25/52999  total_loss: 1.661  loss_cls: 1.647  loss_triplet: 0.01191  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:23:50] fastreid.utils.events INFO:  eta: 7:14:30  epoch/iter: 25/53199  total_loss: 1.661  loss_cls: 1.649  loss_triplet: 0.01266  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:24:17] fastreid.utils.events INFO:  eta: 7:14:23  epoch/iter: 25/53399  total_loss: 1.681  loss_cls: 1.658  loss_triplet: 0.0122  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:24:44] fastreid.utils.events INFO:  eta: 7:14:28  epoch/iter: 25/53599  total_loss: 1.714  loss_cls: 1.694  loss_triplet: 0.01363  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:24:53] fastreid.utils.events INFO:  eta: 7:14:31  epoch/iter: 25/53663  total_loss: 1.726  loss_cls: 1.704  loss_triplet: 0.01374  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:25:11] fastreid.utils.events INFO:  eta: 7:14:44  epoch/iter: 26/53799  total_loss: 1.698  loss_cls: 1.672  loss_triplet: 0.01218  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:25:38] fastreid.utils.events INFO:  eta: 7:14:09  epoch/iter: 26/53999  total_loss: 1.714  loss_cls: 1.696  loss_triplet: 0.0138  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:26:05] fastreid.utils.events INFO:  eta: 7:13:39  epoch/iter: 26/54199  total_loss: 1.712  loss_cls: 1.691  loss_triplet: 0.01416  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:26:32] fastreid.utils.events INFO:  eta: 7:12:47  epoch/iter: 26/54399  total_loss: 1.724  loss_cls: 1.707  loss_triplet: 0.01425  time: 0.1345  data_time: 0.0014  lr: 3.50e-04  max_mem: 8616M
[04/26 23:26:59] fastreid.utils.events INFO:  eta: 7:11:35  epoch/iter: 26/54599  total_loss: 1.646  loss_cls: 1.627  loss_triplet: 0.01257  time: 0.1345  data_time: 0.0010  lr: 3.50e-04  max_mem: 8616M
[04/26 23:27:26] fastreid.utils.events INFO:  eta: 7:10:58  epoch/iter: 26/54799  total_loss: 1.612  loss_cls: 1.595  loss_triplet: 0.01462  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:27:53] fastreid.utils.events INFO:  eta: 7:11:06  epoch/iter: 26/54999  total_loss: 1.665  loss_cls: 1.652  loss_triplet: 0.01318  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:28:20] fastreid.utils.events INFO:  eta: 7:11:07  epoch/iter: 26/55199  total_loss: 1.694  loss_cls: 1.675  loss_triplet: 0.01332  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:28:47] fastreid.utils.events INFO:  eta: 7:11:27  epoch/iter: 26/55399  total_loss: 1.682  loss_cls: 1.667  loss_triplet: 0.01357  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:29:14] fastreid.utils.events INFO:  eta: 7:11:49  epoch/iter: 26/55599  total_loss: 1.692  loss_cls: 1.678  loss_triplet: 0.01409  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:29:31] fastreid.utils.events INFO:  eta: 7:11:43  epoch/iter: 26/55727  total_loss: 1.717  loss_cls: 1.699  loss_triplet: 0.01271  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:29:41] fastreid.utils.events INFO:  eta: 7:11:26  epoch/iter: 27/55799  total_loss: 1.722  loss_cls: 1.708  loss_triplet: 0.01253  time: 0.1345  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 23:30:08] fastreid.utils.events INFO:  eta: 7:10:27  epoch/iter: 27/55999  total_loss: 1.703  loss_cls: 1.683  loss_triplet: 0.01219  time: 0.1345  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 23:30:35] fastreid.utils.events INFO:  eta: 7:09:21  epoch/iter: 27/56199  total_loss: 1.717  loss_cls: 1.701  loss_triplet: 0.01291  time: 0.1345  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 23:31:02] fastreid.utils.events INFO:  eta: 7:08:15  epoch/iter: 27/56399  total_loss: 1.665  loss_cls: 1.647  loss_triplet: 0.01485  time: 0.1345  data_time: 0.0011  lr: 3.50e-04  max_mem: 8616M
[04/26 23:31:28] fastreid.utils.events INFO:  eta: 7:07:02  epoch/iter: 27/56599  total_loss: 1.575  loss_cls: 1.559  loss_triplet: 0.01365  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:31:55] fastreid.utils.events INFO:  eta: 7:06:37  epoch/iter: 27/56799  total_loss: 1.669  loss_cls: 1.657  loss_triplet: 0.01193  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:32:22] fastreid.utils.events INFO:  eta: 7:06:34  epoch/iter: 27/56999  total_loss: 1.691  loss_cls: 1.676  loss_triplet: 0.01186  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:32:49] fastreid.utils.events INFO:  eta: 7:06:39  epoch/iter: 27/57199  total_loss: 1.671  loss_cls: 1.655  loss_triplet: 0.01338  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:33:16] fastreid.utils.events INFO:  eta: 7:07:00  epoch/iter: 27/57399  total_loss: 1.692  loss_cls: 1.671  loss_triplet: 0.01178  time: 0.1345  data_time: 0.0028  lr: 3.50e-04  max_mem: 8616M
[04/26 23:33:43] fastreid.utils.events INFO:  eta: 7:06:56  epoch/iter: 27/57599  total_loss: 1.702  loss_cls: 1.683  loss_triplet: 0.01196  time: 0.1345  data_time: 0.0023  lr: 3.50e-04  max_mem: 8616M
[04/26 23:34:09] fastreid.utils.events INFO:  eta: 7:06:10  epoch/iter: 27/57791  total_loss: 1.706  loss_cls: 1.693  loss_triplet: 0.01412  time: 0.1345  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 23:34:10] fastreid.utils.events INFO:  eta: 7:06:06  epoch/iter: 28/57799  total_loss: 1.706  loss_cls: 1.693  loss_triplet: 0.01399  time: 0.1345  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 23:34:37] fastreid.utils.events INFO:  eta: 7:05:05  epoch/iter: 28/57999  total_loss: 1.682  loss_cls: 1.665  loss_triplet: 0.01472  time: 0.1345  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 23:35:04] fastreid.utils.events INFO:  eta: 7:04:07  epoch/iter: 28/58199  total_loss: 1.665  loss_cls: 1.65  loss_triplet: 0.01419  time: 0.1345  data_time: 0.0016  lr: 3.50e-04  max_mem: 8616M
[04/26 23:35:31] fastreid.utils.events INFO:  eta: 7:02:28  epoch/iter: 28/58399  total_loss: 1.635  loss_cls: 1.62  loss_triplet: 0.01443  time: 0.1345  data_time: 0.0010  lr: 3.50e-04  max_mem: 8616M
[04/26 23:35:58] fastreid.utils.events INFO:  eta: 7:01:49  epoch/iter: 28/58599  total_loss: 1.62  loss_cls: 1.607  loss_triplet: 0.01223  time: 0.1345  data_time: 0.0029  lr: 3.50e-04  max_mem: 8616M
[04/26 23:36:24] fastreid.utils.events INFO:  eta: 7:01:39  epoch/iter: 28/58799  total_loss: 1.657  loss_cls: 1.644  loss_triplet: 0.01211  time: 0.1345  data_time: 0.0032  lr: 3.50e-04  max_mem: 8616M
[04/26 23:36:51] fastreid.utils.events INFO:  eta: 7:01:44  epoch/iter: 28/58999  total_loss: 1.666  loss_cls: 1.649  loss_triplet: 0.0125  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:37:18] fastreid.utils.events INFO:  eta: 7:01:49  epoch/iter: 28/59199  total_loss: 1.675  loss_cls: 1.663  loss_triplet: 0.01341  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:37:45] fastreid.utils.events INFO:  eta: 7:02:16  epoch/iter: 28/59399  total_loss: 1.715  loss_cls: 1.694  loss_triplet: 0.01351  time: 0.1345  data_time: 0.0026  lr: 3.50e-04  max_mem: 8616M
[04/26 23:38:12] fastreid.utils.events INFO:  eta: 7:02:00  epoch/iter: 28/59599  total_loss: 1.729  loss_cls: 1.716  loss_triplet: 0.01256  time: 0.1345  data_time: 0.0021  lr: 3.50e-04  max_mem: 8616M
[04/26 23:38:39] fastreid.utils.events INFO:  eta: 7:01:34  epoch/iter: 28/59799  total_loss: 1.686  loss_cls: 1.669  loss_triplet: 0.01173  time: 0.1345  data_time: 0.0019  lr: 3.50e-04  max_mem: 8616M
[04/26 23:38:47] fastreid.utils.events INFO:  eta: 7:01:06  epoch/iter: 28/59855  total_loss: 1.677  loss_cls: 1.665  loss_triplet: 0.01191  time: 0.1345  data_time: 0.0017  lr: 3.50e-04  max_mem: 8616M
[04/26 23:39:06] fastreid.utils.events INFO:  eta: 7:00:17  epoch/iter: 29/59999  total_loss: 1.683  loss_cls: 1.673  loss_triplet: 0.01215  time: 0.1345  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 23:39:33] fastreid.utils.events INFO:  eta: 6:59:15  epoch/iter: 29/60199  total_loss: 1.642  loss_cls: 1.627  loss_triplet: 0.01197  time: 0.1345  data_time: 0.0015  lr: 3.50e-04  max_mem: 8616M
[04/26 23:40:00] fastreid.utils.events INFO:  eta: 6:58:19  epoch/iter: 29/60399  total_loss: 1.597  loss_cls: 1.575  loss_triplet: 0.01305  time: 0.1345  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 23:40:27] fastreid.utils.events INFO:  eta: 6:57:49  epoch/iter: 29/60599  total_loss: 1.665  loss_cls: 1.641  loss_triplet: 0.01291  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:40:54] fastreid.utils.events INFO:  eta: 6:57:09  epoch/iter: 29/60799  total_loss: 1.677  loss_cls: 1.659  loss_triplet: 0.01295  time: 0.1345  data_time: 0.0025  lr: 3.50e-04  max_mem: 8616M
[04/26 23:41:21] fastreid.utils.events INFO:  eta: 6:57:39  epoch/iter: 29/60999  total_loss: 1.689  loss_cls: 1.671  loss_triplet: 0.01318  time: 0.1345  data_time: 0.0027  lr: 3.50e-04  max_mem: 8616M
[04/26 23:41:48] fastreid.utils.events INFO:  eta: 6:57:55  epoch/iter: 29/61199  total_loss: 1.688  loss_cls: 1.675  loss_triplet: 0.0143  time: 0.1345  data_time: 0.0030  lr: 3.50e-04  max_mem: 8616M
[04/26 23:42:15] fastreid.utils.events INFO:  eta: 6:57:37  epoch/iter: 29/61399  total_loss: 1.684  loss_cls: 1.671  loss_triplet: 0.01206  time: 0.1345  data_time: 0.0024  lr: 3.50e-04  max_mem: 8616M
[04/26 23:42:42] fastreid.utils.events INFO:  eta: 6:57:17  epoch/iter: 29/61599  total_loss: 1.658  loss_cls: 1.641  loss_triplet: 0.01302  time: 0.1345  data_time: 0.0020  lr: 3.50e-04  max_mem: 8616M
[04/26 23:43:08] fastreid.utils.events INFO:  eta: 6:56:42  epoch/iter: 29/61799  total_loss: 1.681  loss_cls: 1.662  loss_triplet: 0.01336  time: 0.1345  data_time: 0.0018  lr: 3.50e-04  max_mem: 8616M
[04/26 23:43:24] fastreid.engine.defaults INFO: Prepare testing set
[04/26 23:43:25] fastreid.data.datasets.bases INFO: => Loaded AIC24 in csv format: 
| subset   | # ids   | # images   | # cameras   |
|:---------|:--------|:-----------|:------------|
| query    | 0       | 0          | 0           |
| gallery  | 0       | 0          | 0           |
[04/26 23:43:25] fastreid.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/train_loop.py", line 148, in train
    self.after_epoch()
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/train_loop.py", line 182, in after_epoch
    h.after_epoch()
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/hooks.py", line 377, in after_epoch
    self._do_eval()
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/hooks.py", line 351, in _do_eval
    results = self._func()
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/defaults.py", line 303, in test_and_save_results
    self._last_eval_results = self.test(self.cfg, self.model)
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/defaults.py", line 432, in test
    data_loader, evaluator = cls.build_evaluator(cfg, dataset_name)
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/defaults.py", line 414, in build_evaluator
    data_loader, num_query = cls.build_test_loader(cfg, dataset_name)
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/engine/defaults.py", line 410, in build_test_loader
    return build_reid_test_loader(cfg, dataset_name=dataset_name)
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/config/config.py", line 265, in wrapped
    return orig_func(**explicit_args)
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/data/build.py", line 155, in build_reid_test_loader
    data_sampler = samplers.InferenceSampler(len(test_set))
  File "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid/data/samplers/data_sampler.py", line 72, in __init__
    assert size > 0
AssertionError
[04/26 23:43:25] fastreid.engine.hooks INFO: Overall training speed: 61918 iterations in 2:18:47 (0.1345 s / it)
[04/26 23:43:25] fastreid.engine.hooks INFO: Total training time: 2:18:58 (0:00:10 on hooks)
[04/26 23:43:35] fastreid INFO: Rank of current process: 0. World size: 1
[04/26 23:43:36] fastreid INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]
numpy                   1.23.5
fastreid                1.3 @/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid
FASTREID_ENV_MODULE     <not set>
PyTorch                 2.0.0+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0                   Tesla V100-PCIE-32GB
CUDA_HOME               /usr/local/cuda
Pillow                  11.1.0
torchvision             0.15.1+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torchvision
torchvision arch flags  sm_35, sm_50, sm_60, sm_70, sm_75, sm_80, sm_86
fvcore                  0.1.5.post20221221
cv2                     4.11.0
----------------------  ----------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/26 23:43:36] fastreid INFO: Command line arguments: Namespace(config_file='configs/jk_experiments/agw-R101-ibn.yml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:60003', opts=[])
[04/26 23:43:36] fastreid INFO: Contents of args.config_file=configs/jk_experiments/agw-R101-ibn.yml:
_BASE_: ../Base-AGW.yml

MODEL:
  BACKBONE:
    DEPTH: 101x
    WITH_IBN: True
  WEIGHTS: "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth"


DATASETS:
  NAMES: ("AIC24",)
  TESTS: ("AIC24",)

OUTPUT_DIR:  logs/jk_experiments/agw-R101-ibn

[04/26 23:43:36] fastreid INFO: Running with full config:
CUDNN_BENCHMARK: True
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER_TRAIN: NaiveIdentitySampler
  SET_WEIGHT: []
DATASETS:
  COMBINEALL: False
  NAMES: ('AIC24',)
  TESTS: ('AIC24',)
INPUT:
  AFFINE:
    ENABLED: False
  AUGMIX:
    ENABLED: False
    PROB: 0.0
  AUTOAUG:
    ENABLED: False
    PROB: 0.0
  CJ:
    BRIGHTNESS: 0.15
    CONTRAST: 0.15
    ENABLED: False
    HUE: 0.1
    PROB: 0.5
    SATURATION: 0.1
  CROP:
    ENABLED: False
    RATIO: [0.75, 1.3333333333333333]
    SCALE: [0.16, 1]
    SIZE: [224, 224]
  FLIP:
    ENABLED: True
    PROB: 0.5
  PADDING:
    ENABLED: True
    MODE: constant
    SIZE: 10
  REA:
    ENABLED: True
    PROB: 0.5
    VALUE: [123.675, 116.28, 103.53]
  RPT:
    ENABLED: False
    PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
KD:
  EMA:
    ENABLED: False
    MOMENTUM: 0.999
  MODEL_CONFIG: []
  MODEL_WEIGHTS: []
MODEL:
  BACKBONE:
    ATT_DROP_RATE: 0.0
    DEPTH: 101x
    DROP_PATH_RATIO: 0.1
    DROP_RATIO: 0.0
    FEAT_DIM: 2048
    LAST_STRIDE: 1
    NAME: build_resnet_backbone
    NORM: BN
    PRETRAIN: True
    PRETRAIN_PATH: 
    SIE_COE: 3.0
    STRIDE_SIZE: (16, 16)
    WITH_IBN: True
    WITH_NL: True
    WITH_SE: False
  DEVICE: cuda
  FREEZE_LAYERS: []
  HEADS:
    CLS_LAYER: Linear
    EMBEDDING_DIM: 0
    MARGIN: 0.0
    NAME: EmbeddingHead
    NECK_FEAT: before
    NORM: BN
    NUM_CLASSES: 0
    POOL_LAYER: GeneralizedMeanPooling
    SCALE: 1
    WITH_BNNECK: True
  LOSSES:
    CE:
      ALPHA: 0.2
      EPSILON: 0.1
      SCALE: 1.0
    CIRCLE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    COSFACE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    FL:
      ALPHA: 0.25
      GAMMA: 2
      SCALE: 1.0
    NAME: ('CrossEntropyLoss', 'TripletLoss')
    TRI:
      HARD_MINING: False
      MARGIN: 0.0
      NORM_FEAT: False
      SCALE: 1.0
  META_ARCHITECTURE: Baseline
  PIXEL_MEAN: [123.675, 116.28, 103.53]
  PIXEL_STD: [58.395, 57.120000000000005, 57.375]
  QUEUE_SIZE: 8192
  WEIGHTS: /WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth
OUTPUT_DIR: logs/jk_experiments/agw-R101-ibn
SOLVER:
  AMP:
    ENABLED: True
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 30
  CLIP_GRADIENTS:
    CLIP_TYPE: norm
    CLIP_VALUE: 5.0
    ENABLED: False
    NORM_TYPE: 2.0
  DELAY_EPOCHS: 0
  ETA_MIN_LR: 1e-07
  FREEZE_ITERS: 0
  GAMMA: 0.1
  HEADS_LR_FACTOR: 1.0
  IMS_PER_BATCH: 64
  MAX_EPOCH: 120
  MOMENTUM: 0.9
  NESTEROV: False
  OPT: Adam
  SCHED: MultiStepLR
  STEPS: [40, 90]
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 2000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005
  WEIGHT_DECAY_NORM: 0.0005
TEST:
  AQE:
    ALPHA: 3.0
    ENABLED: False
    QE_K: 5
    QE_TIME: 1
  EVAL_PERIOD: 30
  FLIP:
    ENABLED: False
  IMS_PER_BATCH: 128
  METRIC: cosine
  PRECISE_BN:
    DATASET: Market1501
    ENABLED: False
    NUM_ITER: 300
  RERANK:
    ENABLED: False
    K1: 20
    K2: 6
    LAMBDA: 0.3
  ROC:
    ENABLED: False
[04/26 23:43:36] fastreid INFO: Full config saved to /WAVE/users2/unix/jkou/PoseTrack/fast-reid/logs/jk_experiments/agw-R101-ibn/config.yaml
[04/26 23:43:36] fastreid.utils.env INFO: Using a generated random seed 40020481
[04/26 23:43:36] fastreid.engine.defaults INFO: Model:
Baseline(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (NL_1): ModuleList()
    (NL_2): ModuleList(
      (0-1): 2 x Non_local(
        (g): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_3): ModuleList(
      (0-8): 9 x Non_local(
        (g): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_4): ModuleList()
  )
  (heads): EmbeddingHead(
    (pool_layer): GeneralizedMeanPooling(3.0, output_size=(1, 1))
    (bottleneck): Sequential(
      (0): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (cls_layer): Linear(num_classes=0, scale=1, margin=0.0)
  )
)
[04/26 23:43:36] fastreid.utils.checkpoint INFO: Loading checkpoint from logs/jk_experiments/agw-R101-ibn/model_best.pth
[04/27 14:13:22] fastreid INFO: Rank of current process: 0. World size: 2
[04/27 14:13:23] fastreid INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]
numpy                   1.23.5
fastreid                1.3 @/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid
FASTREID_ENV_MODULE     <not set>
PyTorch                 2.0.0+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 Tesla V100-PCIE-32GB
CUDA_HOME               /usr/local/cuda
Pillow                  11.1.0
torchvision             0.15.1+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torchvision
torchvision arch flags  sm_35, sm_50, sm_60, sm_70, sm_75, sm_80, sm_86
fvcore                  0.1.5.post20221221
cv2                     4.11.0
----------------------  ----------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/27 14:13:23] fastreid INFO: Command line arguments: Namespace(config_file='./configs/jk_experiments/agw-R101-ibn.yml', resume=False, eval_only=False, num_gpus=2, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:60003', opts=['SOLVER.CHECKPOINT_PERIOD', '1', 'TEST.EVAL_PERIOD', '0'])
[04/27 14:13:23] fastreid INFO: Contents of args.config_file=./configs/jk_experiments/agw-R101-ibn.yml:
_BASE_: ../Base-AGW.yml

MODEL:
  BACKBONE:
    DEPTH: 101x
    WITH_IBN: True
  WEIGHTS: "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth"


DATASETS:
  NAMES: ("AIC24",)
  TESTS: ("AIC24",)

OUTPUT_DIR:  logs/jk_experiments/agw-R101-ibn

[04/27 14:13:23] fastreid INFO: Running with full config:
CUDNN_BENCHMARK: True
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER_TRAIN: NaiveIdentitySampler
  SET_WEIGHT: []
DATASETS:
  COMBINEALL: False
  NAMES: ('AIC24',)
  TESTS: ('AIC24',)
INPUT:
  AFFINE:
    ENABLED: False
  AUGMIX:
    ENABLED: False
    PROB: 0.0
  AUTOAUG:
    ENABLED: False
    PROB: 0.0
  CJ:
    BRIGHTNESS: 0.15
    CONTRAST: 0.15
    ENABLED: False
    HUE: 0.1
    PROB: 0.5
    SATURATION: 0.1
  CROP:
    ENABLED: False
    RATIO: [0.75, 1.3333333333333333]
    SCALE: [0.16, 1]
    SIZE: [224, 224]
  FLIP:
    ENABLED: True
    PROB: 0.5
  PADDING:
    ENABLED: True
    MODE: constant
    SIZE: 10
  REA:
    ENABLED: True
    PROB: 0.5
    VALUE: [123.675, 116.28, 103.53]
  RPT:
    ENABLED: False
    PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
KD:
  EMA:
    ENABLED: False
    MOMENTUM: 0.999
  MODEL_CONFIG: []
  MODEL_WEIGHTS: []
MODEL:
  BACKBONE:
    ATT_DROP_RATE: 0.0
    DEPTH: 101x
    DROP_PATH_RATIO: 0.1
    DROP_RATIO: 0.0
    FEAT_DIM: 2048
    LAST_STRIDE: 1
    NAME: build_resnet_backbone
    NORM: BN
    PRETRAIN: True
    PRETRAIN_PATH: 
    SIE_COE: 3.0
    STRIDE_SIZE: (16, 16)
    WITH_IBN: True
    WITH_NL: True
    WITH_SE: False
  DEVICE: cuda
  FREEZE_LAYERS: []
  HEADS:
    CLS_LAYER: Linear
    EMBEDDING_DIM: 0
    MARGIN: 0.0
    NAME: EmbeddingHead
    NECK_FEAT: before
    NORM: BN
    NUM_CLASSES: 0
    POOL_LAYER: GeneralizedMeanPooling
    SCALE: 1
    WITH_BNNECK: True
  LOSSES:
    CE:
      ALPHA: 0.2
      EPSILON: 0.1
      SCALE: 1.0
    CIRCLE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    COSFACE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    FL:
      ALPHA: 0.25
      GAMMA: 2
      SCALE: 1.0
    NAME: ('CrossEntropyLoss', 'TripletLoss')
    TRI:
      HARD_MINING: False
      MARGIN: 0.0
      NORM_FEAT: False
      SCALE: 1.0
  META_ARCHITECTURE: Baseline
  PIXEL_MEAN: [123.675, 116.28, 103.53]
  PIXEL_STD: [58.395, 57.120000000000005, 57.375]
  QUEUE_SIZE: 8192
  WEIGHTS: /WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth
OUTPUT_DIR: logs/jk_experiments/agw-R101-ibn
SOLVER:
  AMP:
    ENABLED: True
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1
  CLIP_GRADIENTS:
    CLIP_TYPE: norm
    CLIP_VALUE: 5.0
    ENABLED: False
    NORM_TYPE: 2.0
  DELAY_EPOCHS: 0
  ETA_MIN_LR: 1e-07
  FREEZE_ITERS: 0
  GAMMA: 0.1
  HEADS_LR_FACTOR: 1.0
  IMS_PER_BATCH: 64
  MAX_EPOCH: 120
  MOMENTUM: 0.9
  NESTEROV: False
  OPT: Adam
  SCHED: MultiStepLR
  STEPS: [40, 90]
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 2000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005
  WEIGHT_DECAY_NORM: 0.0005
TEST:
  AQE:
    ALPHA: 3.0
    ENABLED: False
    QE_K: 5
    QE_TIME: 1
  EVAL_PERIOD: 0
  FLIP:
    ENABLED: False
  IMS_PER_BATCH: 128
  METRIC: cosine
  PRECISE_BN:
    DATASET: Market1501
    ENABLED: False
    NUM_ITER: 300
  RERANK:
    ENABLED: False
    K1: 20
    K2: 6
    LAMBDA: 0.3
  ROC:
    ENABLED: False
[04/27 14:13:23] fastreid INFO: Full config saved to /WAVE/users2/unix/jkou/PoseTrack/fast-reid/logs/jk_experiments/agw-R101-ibn/config.yaml
[04/27 14:13:23] fastreid.utils.env INFO: Using a generated random seed 26323065
[04/27 14:13:23] fastreid.engine.defaults INFO: Prepare training set
[04/27 14:13:23] fastreid.data.datasets.bases INFO: => Loaded AIC24 in csv format: 
| subset   | # ids   | # images   | # cameras   |
|:---------|:--------|:-----------|:------------|
| train    | 0       | 0          | 0           |
[04/27 14:13:23] fastreid.data.build INFO: Using training sampler NaiveIdentitySampler
[04/27 14:13:23] fastreid.engine.defaults INFO: Auto-scaling the num_classes=0
[04/27 14:13:25] fastreid.modeling.backbones.resnet INFO: Some model parameters or buffers are not found in the checkpoint:
  layer1.0.bn1.IN.{weight, bias}
  layer1.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.1.bn1.IN.{weight, bias}
  layer1.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.2.bn1.IN.{weight, bias}
  layer1.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.0.bn1.IN.{weight, bias}
  layer2.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.1.bn1.IN.{weight, bias}
  layer2.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.2.bn1.IN.{weight, bias}
  layer2.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.3.bn1.IN.{weight, bias}
  layer2.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.0.bn1.IN.{weight, bias}
  layer3.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.1.bn1.IN.{weight, bias}
  layer3.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.2.bn1.IN.{weight, bias}
  layer3.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.3.bn1.IN.{weight, bias}
  layer3.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.4.bn1.IN.{weight, bias}
  layer3.4.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.5.bn1.IN.{weight, bias}
  layer3.5.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.6.bn1.IN.{weight, bias}
  layer3.6.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.7.bn1.IN.{weight, bias}
  layer3.7.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.8.bn1.IN.{weight, bias}
  layer3.8.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.9.bn1.IN.{weight, bias}
  layer3.9.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.10.bn1.IN.{weight, bias}
  layer3.10.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.11.bn1.IN.{weight, bias}
  layer3.11.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.12.bn1.IN.{weight, bias}
  layer3.12.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.13.bn1.IN.{weight, bias}
  layer3.13.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.14.bn1.IN.{weight, bias}
  layer3.14.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.15.bn1.IN.{weight, bias}
  layer3.15.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.16.bn1.IN.{weight, bias}
  layer3.16.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.17.bn1.IN.{weight, bias}
  layer3.17.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.18.bn1.IN.{weight, bias}
  layer3.18.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.19.bn1.IN.{weight, bias}
  layer3.19.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.20.bn1.IN.{weight, bias}
  layer3.20.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.21.bn1.IN.{weight, bias}
  layer3.21.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.22.bn1.IN.{weight, bias}
  layer3.22.bn1.BN.{weight, bias, running_mean, running_var}
  NL_2.0.g.{weight, bias}
  NL_2.0.W.0.{weight, bias}
  NL_2.0.W.1.{weight, bias, running_mean, running_var}
  NL_2.0.theta.{weight, bias}
  NL_2.0.phi.{weight, bias}
  NL_2.1.g.{weight, bias}
  NL_2.1.W.0.{weight, bias}
  NL_2.1.W.1.{weight, bias, running_mean, running_var}
  NL_2.1.theta.{weight, bias}
  NL_2.1.phi.{weight, bias}
  NL_3.0.g.{weight, bias}
  NL_3.0.W.0.{weight, bias}
  NL_3.0.W.1.{weight, bias, running_mean, running_var}
  NL_3.0.theta.{weight, bias}
  NL_3.0.phi.{weight, bias}
  NL_3.1.g.{weight, bias}
  NL_3.1.W.0.{weight, bias}
  NL_3.1.W.1.{weight, bias, running_mean, running_var}
  NL_3.1.theta.{weight, bias}
  NL_3.1.phi.{weight, bias}
  NL_3.2.g.{weight, bias}
  NL_3.2.W.0.{weight, bias}
  NL_3.2.W.1.{weight, bias, running_mean, running_var}
  NL_3.2.theta.{weight, bias}
  NL_3.2.phi.{weight, bias}
  NL_3.3.g.{weight, bias}
  NL_3.3.W.0.{weight, bias}
  NL_3.3.W.1.{weight, bias, running_mean, running_var}
  NL_3.3.theta.{weight, bias}
  NL_3.3.phi.{weight, bias}
  NL_3.4.g.{weight, bias}
  NL_3.4.W.0.{weight, bias}
  NL_3.4.W.1.{weight, bias, running_mean, running_var}
  NL_3.4.theta.{weight, bias}
  NL_3.4.phi.{weight, bias}
  NL_3.5.g.{weight, bias}
  NL_3.5.W.0.{weight, bias}
  NL_3.5.W.1.{weight, bias, running_mean, running_var}
  NL_3.5.theta.{weight, bias}
  NL_3.5.phi.{weight, bias}
  NL_3.6.g.{weight, bias}
  NL_3.6.W.0.{weight, bias}
  NL_3.6.W.1.{weight, bias, running_mean, running_var}
  NL_3.6.theta.{weight, bias}
  NL_3.6.phi.{weight, bias}
  NL_3.7.g.{weight, bias}
  NL_3.7.W.0.{weight, bias}
  NL_3.7.W.1.{weight, bias, running_mean, running_var}
  NL_3.7.theta.{weight, bias}
  NL_3.7.phi.{weight, bias}
  NL_3.8.g.{weight, bias}
  NL_3.8.W.0.{weight, bias}
  NL_3.8.W.1.{weight, bias, running_mean, running_var}
  NL_3.8.theta.{weight, bias}
  NL_3.8.phi.{weight, bias}
[04/27 14:13:25] fastreid.modeling.backbones.resnet INFO: The checkpoint state_dict contains keys that are not used by the model:
  layer1.0.bn1.{weight, bias, running_mean, running_var}
  layer1.1.bn1.{weight, bias, running_mean, running_var}
  layer1.2.bn1.{weight, bias, running_mean, running_var}
  layer2.0.bn1.{weight, bias, running_mean, running_var}
  layer2.1.bn1.{weight, bias, running_mean, running_var}
  layer2.2.bn1.{weight, bias, running_mean, running_var}
  layer2.3.bn1.{weight, bias, running_mean, running_var}
  layer3.0.bn1.{weight, bias, running_mean, running_var}
  layer3.1.bn1.{weight, bias, running_mean, running_var}
  layer3.2.bn1.{weight, bias, running_mean, running_var}
  layer3.3.bn1.{weight, bias, running_mean, running_var}
  layer3.4.bn1.{weight, bias, running_mean, running_var}
  layer3.5.bn1.{weight, bias, running_mean, running_var}
  layer3.6.bn1.{weight, bias, running_mean, running_var}
  layer3.7.bn1.{weight, bias, running_mean, running_var}
  layer3.8.bn1.{weight, bias, running_mean, running_var}
  layer3.9.bn1.{weight, bias, running_mean, running_var}
  layer3.10.bn1.{weight, bias, running_mean, running_var}
  layer3.11.bn1.{weight, bias, running_mean, running_var}
  layer3.12.bn1.{weight, bias, running_mean, running_var}
  layer3.13.bn1.{weight, bias, running_mean, running_var}
  layer3.14.bn1.{weight, bias, running_mean, running_var}
  layer3.15.bn1.{weight, bias, running_mean, running_var}
  layer3.16.bn1.{weight, bias, running_mean, running_var}
  layer3.17.bn1.{weight, bias, running_mean, running_var}
  layer3.18.bn1.{weight, bias, running_mean, running_var}
  layer3.19.bn1.{weight, bias, running_mean, running_var}
  layer3.20.bn1.{weight, bias, running_mean, running_var}
  layer3.21.bn1.{weight, bias, running_mean, running_var}
  layer3.22.bn1.{weight, bias, running_mean, running_var}
[04/27 14:13:25] fastreid.engine.defaults INFO: Model:
Baseline(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (NL_1): ModuleList()
    (NL_2): ModuleList(
      (0-1): 2 x Non_local(
        (g): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_3): ModuleList(
      (0-8): 9 x Non_local(
        (g): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_4): ModuleList()
  )
  (heads): EmbeddingHead(
    (pool_layer): GeneralizedMeanPooling(3.0, output_size=(1, 1))
    (bottleneck): Sequential(
      (0): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (cls_layer): Linear(num_classes=0, scale=1, margin=0.0)
  )
)
[04/27 14:43:42] fastreid INFO: Rank of current process: 0. World size: 2
[04/27 14:43:43] fastreid INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]
numpy                   1.23.5
fastreid                1.3 @/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid
FASTREID_ENV_MODULE     <not set>
PyTorch                 2.0.0+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 Tesla V100-PCIE-32GB
CUDA_HOME               /usr/local/cuda
Pillow                  11.1.0
torchvision             0.15.1+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torchvision
torchvision arch flags  sm_35, sm_50, sm_60, sm_70, sm_75, sm_80, sm_86
fvcore                  0.1.5.post20221221
cv2                     4.11.0
----------------------  ----------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/27 14:43:43] fastreid INFO: Command line arguments: Namespace(config_file='./configs/jk_experiments/agw-R101-ibn.yml', resume=False, eval_only=False, num_gpus=2, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:60003', opts=['SOLVER.CHECKPOINT_PERIOD', '1', 'TEST.EVAL_PERIOD', '1'])
[04/27 14:43:43] fastreid INFO: Contents of args.config_file=./configs/jk_experiments/agw-R101-ibn.yml:
_BASE_: ../Base-AGW.yml

MODEL:
  BACKBONE:
    DEPTH: 101x
    WITH_IBN: True
  WEIGHTS: "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth"


DATASETS:
  NAMES: ("AIC24",)
  TESTS: ("AIC24",)

OUTPUT_DIR:  logs/jk_experiments/agw-R101-ibn

[04/27 14:43:43] fastreid INFO: Running with full config:
CUDNN_BENCHMARK: True
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER_TRAIN: NaiveIdentitySampler
  SET_WEIGHT: []
DATASETS:
  COMBINEALL: False
  NAMES: ('AIC24',)
  TESTS: ('AIC24',)
INPUT:
  AFFINE:
    ENABLED: False
  AUGMIX:
    ENABLED: False
    PROB: 0.0
  AUTOAUG:
    ENABLED: False
    PROB: 0.0
  CJ:
    BRIGHTNESS: 0.15
    CONTRAST: 0.15
    ENABLED: False
    HUE: 0.1
    PROB: 0.5
    SATURATION: 0.1
  CROP:
    ENABLED: False
    RATIO: [0.75, 1.3333333333333333]
    SCALE: [0.16, 1]
    SIZE: [224, 224]
  FLIP:
    ENABLED: True
    PROB: 0.5
  PADDING:
    ENABLED: True
    MODE: constant
    SIZE: 10
  REA:
    ENABLED: True
    PROB: 0.5
    VALUE: [123.675, 116.28, 103.53]
  RPT:
    ENABLED: False
    PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
KD:
  EMA:
    ENABLED: False
    MOMENTUM: 0.999
  MODEL_CONFIG: []
  MODEL_WEIGHTS: []
MODEL:
  BACKBONE:
    ATT_DROP_RATE: 0.0
    DEPTH: 101x
    DROP_PATH_RATIO: 0.1
    DROP_RATIO: 0.0
    FEAT_DIM: 2048
    LAST_STRIDE: 1
    NAME: build_resnet_backbone
    NORM: BN
    PRETRAIN: True
    PRETRAIN_PATH: 
    SIE_COE: 3.0
    STRIDE_SIZE: (16, 16)
    WITH_IBN: True
    WITH_NL: True
    WITH_SE: False
  DEVICE: cuda
  FREEZE_LAYERS: []
  HEADS:
    CLS_LAYER: Linear
    EMBEDDING_DIM: 0
    MARGIN: 0.0
    NAME: EmbeddingHead
    NECK_FEAT: before
    NORM: BN
    NUM_CLASSES: 0
    POOL_LAYER: GeneralizedMeanPooling
    SCALE: 1
    WITH_BNNECK: True
  LOSSES:
    CE:
      ALPHA: 0.2
      EPSILON: 0.1
      SCALE: 1.0
    CIRCLE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    COSFACE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    FL:
      ALPHA: 0.25
      GAMMA: 2
      SCALE: 1.0
    NAME: ('CrossEntropyLoss', 'TripletLoss')
    TRI:
      HARD_MINING: False
      MARGIN: 0.0
      NORM_FEAT: False
      SCALE: 1.0
  META_ARCHITECTURE: Baseline
  PIXEL_MEAN: [123.675, 116.28, 103.53]
  PIXEL_STD: [58.395, 57.120000000000005, 57.375]
  QUEUE_SIZE: 8192
  WEIGHTS: /WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/market_agw_R101-ibn.pth
OUTPUT_DIR: logs/jk_experiments/agw-R101-ibn
SOLVER:
  AMP:
    ENABLED: True
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1
  CLIP_GRADIENTS:
    CLIP_TYPE: norm
    CLIP_VALUE: 5.0
    ENABLED: False
    NORM_TYPE: 2.0
  DELAY_EPOCHS: 0
  ETA_MIN_LR: 1e-07
  FREEZE_ITERS: 0
  GAMMA: 0.1
  HEADS_LR_FACTOR: 1.0
  IMS_PER_BATCH: 64
  MAX_EPOCH: 120
  MOMENTUM: 0.9
  NESTEROV: False
  OPT: Adam
  SCHED: MultiStepLR
  STEPS: [40, 90]
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 2000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005
  WEIGHT_DECAY_NORM: 0.0005
TEST:
  AQE:
    ALPHA: 3.0
    ENABLED: False
    QE_K: 5
    QE_TIME: 1
  EVAL_PERIOD: 1
  FLIP:
    ENABLED: False
  IMS_PER_BATCH: 128
  METRIC: cosine
  PRECISE_BN:
    DATASET: Market1501
    ENABLED: False
    NUM_ITER: 300
  RERANK:
    ENABLED: False
    K1: 20
    K2: 6
    LAMBDA: 0.3
  ROC:
    ENABLED: False
[04/27 14:43:43] fastreid INFO: Full config saved to /WAVE/users2/unix/jkou/PoseTrack/fast-reid/logs/jk_experiments/agw-R101-ibn/config.yaml
[04/27 14:43:43] fastreid.utils.env INFO: Using a generated random seed 46854875
[04/27 14:43:43] fastreid.engine.defaults INFO: Prepare training set
[04/27 14:43:43] fastreid.data.datasets.bases INFO: => Loaded AIC24 in csv format: 
| subset   | # ids   | # images   | # cameras   |
|:---------|:--------|:-----------|:------------|
| train    | 0       | 0          | 0           |
[04/27 14:43:43] fastreid.data.build INFO: Using training sampler NaiveIdentitySampler
[04/27 14:43:43] fastreid.engine.defaults INFO: Auto-scaling the num_classes=0
[04/27 14:43:45] fastreid.modeling.backbones.resnet INFO: Some model parameters or buffers are not found in the checkpoint:
  layer1.0.bn1.IN.{weight, bias}
  layer1.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.1.bn1.IN.{weight, bias}
  layer1.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.2.bn1.IN.{weight, bias}
  layer1.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.0.bn1.IN.{weight, bias}
  layer2.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.1.bn1.IN.{weight, bias}
  layer2.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.2.bn1.IN.{weight, bias}
  layer2.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.3.bn1.IN.{weight, bias}
  layer2.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.0.bn1.IN.{weight, bias}
  layer3.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.1.bn1.IN.{weight, bias}
  layer3.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.2.bn1.IN.{weight, bias}
  layer3.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.3.bn1.IN.{weight, bias}
  layer3.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.4.bn1.IN.{weight, bias}
  layer3.4.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.5.bn1.IN.{weight, bias}
  layer3.5.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.6.bn1.IN.{weight, bias}
  layer3.6.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.7.bn1.IN.{weight, bias}
  layer3.7.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.8.bn1.IN.{weight, bias}
  layer3.8.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.9.bn1.IN.{weight, bias}
  layer3.9.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.10.bn1.IN.{weight, bias}
  layer3.10.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.11.bn1.IN.{weight, bias}
  layer3.11.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.12.bn1.IN.{weight, bias}
  layer3.12.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.13.bn1.IN.{weight, bias}
  layer3.13.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.14.bn1.IN.{weight, bias}
  layer3.14.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.15.bn1.IN.{weight, bias}
  layer3.15.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.16.bn1.IN.{weight, bias}
  layer3.16.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.17.bn1.IN.{weight, bias}
  layer3.17.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.18.bn1.IN.{weight, bias}
  layer3.18.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.19.bn1.IN.{weight, bias}
  layer3.19.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.20.bn1.IN.{weight, bias}
  layer3.20.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.21.bn1.IN.{weight, bias}
  layer3.21.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.22.bn1.IN.{weight, bias}
  layer3.22.bn1.BN.{weight, bias, running_mean, running_var}
  NL_2.0.g.{weight, bias}
  NL_2.0.W.0.{weight, bias}
  NL_2.0.W.1.{weight, bias, running_mean, running_var}
  NL_2.0.theta.{weight, bias}
  NL_2.0.phi.{weight, bias}
  NL_2.1.g.{weight, bias}
  NL_2.1.W.0.{weight, bias}
  NL_2.1.W.1.{weight, bias, running_mean, running_var}
  NL_2.1.theta.{weight, bias}
  NL_2.1.phi.{weight, bias}
  NL_3.0.g.{weight, bias}
  NL_3.0.W.0.{weight, bias}
  NL_3.0.W.1.{weight, bias, running_mean, running_var}
  NL_3.0.theta.{weight, bias}
  NL_3.0.phi.{weight, bias}
  NL_3.1.g.{weight, bias}
  NL_3.1.W.0.{weight, bias}
  NL_3.1.W.1.{weight, bias, running_mean, running_var}
  NL_3.1.theta.{weight, bias}
  NL_3.1.phi.{weight, bias}
  NL_3.2.g.{weight, bias}
  NL_3.2.W.0.{weight, bias}
  NL_3.2.W.1.{weight, bias, running_mean, running_var}
  NL_3.2.theta.{weight, bias}
  NL_3.2.phi.{weight, bias}
  NL_3.3.g.{weight, bias}
  NL_3.3.W.0.{weight, bias}
  NL_3.3.W.1.{weight, bias, running_mean, running_var}
  NL_3.3.theta.{weight, bias}
  NL_3.3.phi.{weight, bias}
  NL_3.4.g.{weight, bias}
  NL_3.4.W.0.{weight, bias}
  NL_3.4.W.1.{weight, bias, running_mean, running_var}
  NL_3.4.theta.{weight, bias}
  NL_3.4.phi.{weight, bias}
  NL_3.5.g.{weight, bias}
  NL_3.5.W.0.{weight, bias}
  NL_3.5.W.1.{weight, bias, running_mean, running_var}
  NL_3.5.theta.{weight, bias}
  NL_3.5.phi.{weight, bias}
  NL_3.6.g.{weight, bias}
  NL_3.6.W.0.{weight, bias}
  NL_3.6.W.1.{weight, bias, running_mean, running_var}
  NL_3.6.theta.{weight, bias}
  NL_3.6.phi.{weight, bias}
  NL_3.7.g.{weight, bias}
  NL_3.7.W.0.{weight, bias}
  NL_3.7.W.1.{weight, bias, running_mean, running_var}
  NL_3.7.theta.{weight, bias}
  NL_3.7.phi.{weight, bias}
  NL_3.8.g.{weight, bias}
  NL_3.8.W.0.{weight, bias}
  NL_3.8.W.1.{weight, bias, running_mean, running_var}
  NL_3.8.theta.{weight, bias}
  NL_3.8.phi.{weight, bias}
[04/27 14:43:45] fastreid.modeling.backbones.resnet INFO: The checkpoint state_dict contains keys that are not used by the model:
  layer1.0.bn1.{weight, bias, running_mean, running_var}
  layer1.1.bn1.{weight, bias, running_mean, running_var}
  layer1.2.bn1.{weight, bias, running_mean, running_var}
  layer2.0.bn1.{weight, bias, running_mean, running_var}
  layer2.1.bn1.{weight, bias, running_mean, running_var}
  layer2.2.bn1.{weight, bias, running_mean, running_var}
  layer2.3.bn1.{weight, bias, running_mean, running_var}
  layer3.0.bn1.{weight, bias, running_mean, running_var}
  layer3.1.bn1.{weight, bias, running_mean, running_var}
  layer3.2.bn1.{weight, bias, running_mean, running_var}
  layer3.3.bn1.{weight, bias, running_mean, running_var}
  layer3.4.bn1.{weight, bias, running_mean, running_var}
  layer3.5.bn1.{weight, bias, running_mean, running_var}
  layer3.6.bn1.{weight, bias, running_mean, running_var}
  layer3.7.bn1.{weight, bias, running_mean, running_var}
  layer3.8.bn1.{weight, bias, running_mean, running_var}
  layer3.9.bn1.{weight, bias, running_mean, running_var}
  layer3.10.bn1.{weight, bias, running_mean, running_var}
  layer3.11.bn1.{weight, bias, running_mean, running_var}
  layer3.12.bn1.{weight, bias, running_mean, running_var}
  layer3.13.bn1.{weight, bias, running_mean, running_var}
  layer3.14.bn1.{weight, bias, running_mean, running_var}
  layer3.15.bn1.{weight, bias, running_mean, running_var}
  layer3.16.bn1.{weight, bias, running_mean, running_var}
  layer3.17.bn1.{weight, bias, running_mean, running_var}
  layer3.18.bn1.{weight, bias, running_mean, running_var}
  layer3.19.bn1.{weight, bias, running_mean, running_var}
  layer3.20.bn1.{weight, bias, running_mean, running_var}
  layer3.21.bn1.{weight, bias, running_mean, running_var}
  layer3.22.bn1.{weight, bias, running_mean, running_var}
[04/27 14:43:45] fastreid.engine.defaults INFO: Model:
Baseline(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (NL_1): ModuleList()
    (NL_2): ModuleList(
      (0-1): 2 x Non_local(
        (g): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_3): ModuleList(
      (0-8): 9 x Non_local(
        (g): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_4): ModuleList()
  )
  (heads): EmbeddingHead(
    (pool_layer): GeneralizedMeanPooling(3.0, output_size=(1, 1))
    (bottleneck): Sequential(
      (0): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (cls_layer): Linear(num_classes=0, scale=1, margin=0.0)
  )
)
[04/27 22:27:03] fastreid INFO: Rank of current process: 0. World size: 2
[04/27 22:27:04] fastreid INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]
numpy                   1.23.5
fastreid                1.3 @/WAVE/users2/unix/jkou/PoseTrack/fast-reid/./fastreid
FASTREID_ENV_MODULE     <not set>
PyTorch                 2.0.0+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 Tesla V100-PCIE-32GB
CUDA_HOME               /usr/local/cuda
Pillow                  11.1.0
torchvision             0.15.1+cu118 @/WAVE/users2/unix/jkou/.conda/envs/aic24/lib/python3.9/site-packages/torchvision
torchvision arch flags  sm_35, sm_50, sm_60, sm_70, sm_75, sm_80, sm_86
fvcore                  0.1.5.post20221221
cv2                     4.11.0
----------------------  ----------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/27 22:27:04] fastreid INFO: Command line arguments: Namespace(config_file='./configs/jk_experiments/agw-R101-ibn.yml', resume=False, eval_only=False, num_gpus=2, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:60003', opts=['SOLVER.CHECKPOINT_PERIOD', '1', 'TEST.EVAL_PERIOD', '1'])
[04/27 22:27:04] fastreid INFO: Contents of args.config_file=./configs/jk_experiments/agw-R101-ibn.yml:
_BASE_: ../Base-AGW.yml

MODEL:
  BACKBONE:
    DEPTH: 101x
    WITH_IBN: True
  WEIGHTS: "/WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/duke_agw_R101-ibn.pth"


DATASETS:
  NAMES: ("AIC24",)
  TESTS: ("AIC24",)

OUTPUT_DIR:  logs/jk_experiments/agw-R101-ibn

[04/27 22:27:04] fastreid INFO: Running with full config:
CUDNN_BENCHMARK: True
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER_TRAIN: NaiveIdentitySampler
  SET_WEIGHT: []
DATASETS:
  COMBINEALL: False
  NAMES: ('AIC24',)
  TESTS: ('AIC24',)
INPUT:
  AFFINE:
    ENABLED: False
  AUGMIX:
    ENABLED: False
    PROB: 0.0
  AUTOAUG:
    ENABLED: False
    PROB: 0.0
  CJ:
    BRIGHTNESS: 0.15
    CONTRAST: 0.15
    ENABLED: False
    HUE: 0.1
    PROB: 0.5
    SATURATION: 0.1
  CROP:
    ENABLED: False
    RATIO: [0.75, 1.3333333333333333]
    SCALE: [0.16, 1]
    SIZE: [224, 224]
  FLIP:
    ENABLED: True
    PROB: 0.5
  PADDING:
    ENABLED: True
    MODE: constant
    SIZE: 10
  REA:
    ENABLED: True
    PROB: 0.5
    VALUE: [123.675, 116.28, 103.53]
  RPT:
    ENABLED: False
    PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
KD:
  EMA:
    ENABLED: False
    MOMENTUM: 0.999
  MODEL_CONFIG: []
  MODEL_WEIGHTS: []
MODEL:
  BACKBONE:
    ATT_DROP_RATE: 0.0
    DEPTH: 101x
    DROP_PATH_RATIO: 0.1
    DROP_RATIO: 0.0
    FEAT_DIM: 2048
    LAST_STRIDE: 1
    NAME: build_resnet_backbone
    NORM: BN
    PRETRAIN: True
    PRETRAIN_PATH: 
    SIE_COE: 3.0
    STRIDE_SIZE: (16, 16)
    WITH_IBN: True
    WITH_NL: True
    WITH_SE: False
  DEVICE: cuda
  FREEZE_LAYERS: []
  HEADS:
    CLS_LAYER: Linear
    EMBEDDING_DIM: 0
    MARGIN: 0.0
    NAME: EmbeddingHead
    NECK_FEAT: before
    NORM: BN
    NUM_CLASSES: 0
    POOL_LAYER: GeneralizedMeanPooling
    SCALE: 1
    WITH_BNNECK: True
  LOSSES:
    CE:
      ALPHA: 0.2
      EPSILON: 0.1
      SCALE: 1.0
    CIRCLE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    COSFACE:
      GAMMA: 128
      MARGIN: 0.25
      SCALE: 1.0
    FL:
      ALPHA: 0.25
      GAMMA: 2
      SCALE: 1.0
    NAME: ('CrossEntropyLoss', 'TripletLoss')
    TRI:
      HARD_MINING: False
      MARGIN: 0.0
      NORM_FEAT: False
      SCALE: 1.0
  META_ARCHITECTURE: Baseline
  PIXEL_MEAN: [123.675, 116.28, 103.53]
  PIXEL_STD: [58.395, 57.120000000000005, 57.375]
  QUEUE_SIZE: 8192
  WEIGHTS: /WAVE/users2/unix/jkou/PoseTrack/fast-reid/fastreid/config/duke_agw_R101-ibn.pth
OUTPUT_DIR: logs/jk_experiments/agw-R101-ibn
SOLVER:
  AMP:
    ENABLED: True
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1
  CLIP_GRADIENTS:
    CLIP_TYPE: norm
    CLIP_VALUE: 5.0
    ENABLED: False
    NORM_TYPE: 2.0
  DELAY_EPOCHS: 0
  ETA_MIN_LR: 1e-07
  FREEZE_ITERS: 0
  GAMMA: 0.1
  HEADS_LR_FACTOR: 1.0
  IMS_PER_BATCH: 64
  MAX_EPOCH: 120
  MOMENTUM: 0.9
  NESTEROV: False
  OPT: Adam
  SCHED: MultiStepLR
  STEPS: [40, 90]
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 2000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0005
  WEIGHT_DECAY_NORM: 0.0005
TEST:
  AQE:
    ALPHA: 3.0
    ENABLED: False
    QE_K: 5
    QE_TIME: 1
  EVAL_PERIOD: 1
  FLIP:
    ENABLED: False
  IMS_PER_BATCH: 128
  METRIC: cosine
  PRECISE_BN:
    DATASET: Market1501
    ENABLED: False
    NUM_ITER: 300
  RERANK:
    ENABLED: False
    K1: 20
    K2: 6
    LAMBDA: 0.3
  ROC:
    ENABLED: False
[04/27 22:27:04] fastreid INFO: Full config saved to /WAVE/users2/unix/jkou/PoseTrack/fast-reid/logs/jk_experiments/agw-R101-ibn/config.yaml
[04/27 22:27:04] fastreid.utils.env INFO: Using a generated random seed 6171967
[04/27 22:27:04] fastreid.engine.defaults INFO: Prepare training set
[04/27 22:27:04] fastreid.data.datasets.bases INFO: => Loaded AIC24 in csv format: 
| subset   | # ids   | # images   | # cameras   |
|:---------|:--------|:-----------|:------------|
| train    | 0       | 0          | 0           |
[04/27 22:27:04] fastreid.data.build INFO: Using training sampler NaiveIdentitySampler
[04/27 22:27:04] fastreid.engine.defaults INFO: Auto-scaling the num_classes=0
[04/27 22:27:06] fastreid.modeling.backbones.resnet INFO: Some model parameters or buffers are not found in the checkpoint:
  layer1.0.bn1.IN.{weight, bias}
  layer1.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.1.bn1.IN.{weight, bias}
  layer1.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer1.2.bn1.IN.{weight, bias}
  layer1.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.0.bn1.IN.{weight, bias}
  layer2.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.1.bn1.IN.{weight, bias}
  layer2.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.2.bn1.IN.{weight, bias}
  layer2.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer2.3.bn1.IN.{weight, bias}
  layer2.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.0.bn1.IN.{weight, bias}
  layer3.0.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.1.bn1.IN.{weight, bias}
  layer3.1.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.2.bn1.IN.{weight, bias}
  layer3.2.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.3.bn1.IN.{weight, bias}
  layer3.3.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.4.bn1.IN.{weight, bias}
  layer3.4.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.5.bn1.IN.{weight, bias}
  layer3.5.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.6.bn1.IN.{weight, bias}
  layer3.6.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.7.bn1.IN.{weight, bias}
  layer3.7.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.8.bn1.IN.{weight, bias}
  layer3.8.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.9.bn1.IN.{weight, bias}
  layer3.9.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.10.bn1.IN.{weight, bias}
  layer3.10.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.11.bn1.IN.{weight, bias}
  layer3.11.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.12.bn1.IN.{weight, bias}
  layer3.12.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.13.bn1.IN.{weight, bias}
  layer3.13.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.14.bn1.IN.{weight, bias}
  layer3.14.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.15.bn1.IN.{weight, bias}
  layer3.15.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.16.bn1.IN.{weight, bias}
  layer3.16.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.17.bn1.IN.{weight, bias}
  layer3.17.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.18.bn1.IN.{weight, bias}
  layer3.18.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.19.bn1.IN.{weight, bias}
  layer3.19.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.20.bn1.IN.{weight, bias}
  layer3.20.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.21.bn1.IN.{weight, bias}
  layer3.21.bn1.BN.{weight, bias, running_mean, running_var}
  layer3.22.bn1.IN.{weight, bias}
  layer3.22.bn1.BN.{weight, bias, running_mean, running_var}
  NL_2.0.g.{weight, bias}
  NL_2.0.W.0.{weight, bias}
  NL_2.0.W.1.{weight, bias, running_mean, running_var}
  NL_2.0.theta.{weight, bias}
  NL_2.0.phi.{weight, bias}
  NL_2.1.g.{weight, bias}
  NL_2.1.W.0.{weight, bias}
  NL_2.1.W.1.{weight, bias, running_mean, running_var}
  NL_2.1.theta.{weight, bias}
  NL_2.1.phi.{weight, bias}
  NL_3.0.g.{weight, bias}
  NL_3.0.W.0.{weight, bias}
  NL_3.0.W.1.{weight, bias, running_mean, running_var}
  NL_3.0.theta.{weight, bias}
  NL_3.0.phi.{weight, bias}
  NL_3.1.g.{weight, bias}
  NL_3.1.W.0.{weight, bias}
  NL_3.1.W.1.{weight, bias, running_mean, running_var}
  NL_3.1.theta.{weight, bias}
  NL_3.1.phi.{weight, bias}
  NL_3.2.g.{weight, bias}
  NL_3.2.W.0.{weight, bias}
  NL_3.2.W.1.{weight, bias, running_mean, running_var}
  NL_3.2.theta.{weight, bias}
  NL_3.2.phi.{weight, bias}
  NL_3.3.g.{weight, bias}
  NL_3.3.W.0.{weight, bias}
  NL_3.3.W.1.{weight, bias, running_mean, running_var}
  NL_3.3.theta.{weight, bias}
  NL_3.3.phi.{weight, bias}
  NL_3.4.g.{weight, bias}
  NL_3.4.W.0.{weight, bias}
  NL_3.4.W.1.{weight, bias, running_mean, running_var}
  NL_3.4.theta.{weight, bias}
  NL_3.4.phi.{weight, bias}
  NL_3.5.g.{weight, bias}
  NL_3.5.W.0.{weight, bias}
  NL_3.5.W.1.{weight, bias, running_mean, running_var}
  NL_3.5.theta.{weight, bias}
  NL_3.5.phi.{weight, bias}
  NL_3.6.g.{weight, bias}
  NL_3.6.W.0.{weight, bias}
  NL_3.6.W.1.{weight, bias, running_mean, running_var}
  NL_3.6.theta.{weight, bias}
  NL_3.6.phi.{weight, bias}
  NL_3.7.g.{weight, bias}
  NL_3.7.W.0.{weight, bias}
  NL_3.7.W.1.{weight, bias, running_mean, running_var}
  NL_3.7.theta.{weight, bias}
  NL_3.7.phi.{weight, bias}
  NL_3.8.g.{weight, bias}
  NL_3.8.W.0.{weight, bias}
  NL_3.8.W.1.{weight, bias, running_mean, running_var}
  NL_3.8.theta.{weight, bias}
  NL_3.8.phi.{weight, bias}
[04/27 22:27:06] fastreid.modeling.backbones.resnet INFO: The checkpoint state_dict contains keys that are not used by the model:
  layer1.0.bn1.{weight, bias, running_mean, running_var}
  layer1.1.bn1.{weight, bias, running_mean, running_var}
  layer1.2.bn1.{weight, bias, running_mean, running_var}
  layer2.0.bn1.{weight, bias, running_mean, running_var}
  layer2.1.bn1.{weight, bias, running_mean, running_var}
  layer2.2.bn1.{weight, bias, running_mean, running_var}
  layer2.3.bn1.{weight, bias, running_mean, running_var}
  layer3.0.bn1.{weight, bias, running_mean, running_var}
  layer3.1.bn1.{weight, bias, running_mean, running_var}
  layer3.2.bn1.{weight, bias, running_mean, running_var}
  layer3.3.bn1.{weight, bias, running_mean, running_var}
  layer3.4.bn1.{weight, bias, running_mean, running_var}
  layer3.5.bn1.{weight, bias, running_mean, running_var}
  layer3.6.bn1.{weight, bias, running_mean, running_var}
  layer3.7.bn1.{weight, bias, running_mean, running_var}
  layer3.8.bn1.{weight, bias, running_mean, running_var}
  layer3.9.bn1.{weight, bias, running_mean, running_var}
  layer3.10.bn1.{weight, bias, running_mean, running_var}
  layer3.11.bn1.{weight, bias, running_mean, running_var}
  layer3.12.bn1.{weight, bias, running_mean, running_var}
  layer3.13.bn1.{weight, bias, running_mean, running_var}
  layer3.14.bn1.{weight, bias, running_mean, running_var}
  layer3.15.bn1.{weight, bias, running_mean, running_var}
  layer3.16.bn1.{weight, bias, running_mean, running_var}
  layer3.17.bn1.{weight, bias, running_mean, running_var}
  layer3.18.bn1.{weight, bias, running_mean, running_var}
  layer3.19.bn1.{weight, bias, running_mean, running_var}
  layer3.20.bn1.{weight, bias, running_mean, running_var}
  layer3.21.bn1.{weight, bias, running_mean, running_var}
  layer3.22.bn1.{weight, bias, running_mean, running_var}
[04/27 22:27:06] fastreid.engine.defaults INFO: Model:
Baseline(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): IBN(
          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (se): Identity()
      )
    )
    (NL_1): ModuleList()
    (NL_2): ModuleList(
      (0-1): 2 x Non_local(
        (g): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 512, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_3): ModuleList(
      (0-8): 9 x Non_local(
        (g): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (W): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (theta): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (phi): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (NL_4): ModuleList()
  )
  (heads): EmbeddingHead(
    (pool_layer): GeneralizedMeanPooling(3.0, output_size=(1, 1))
    (bottleneck): Sequential(
      (0): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (cls_layer): Linear(num_classes=0, scale=1, margin=0.0)
  )
)
